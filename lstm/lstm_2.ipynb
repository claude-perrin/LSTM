{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2894677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#  import matplotlib.pyplot as plt\n",
    "from activation_functions import softmax\n",
    "from tensorflow.keras import activations\n",
    "tf_sigmoid = activations.sigmoid\n",
    "def sigmoid(X):\n",
    "    return tf_sigmoid(X).numpy()\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x * (1 - sigmoid_x)\n",
    "\n",
    "def tanh_gradient(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50414f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true, epsilon=1e-10):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    num_samples = y_pred.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon)) / num_samples\n",
    "    return loss\n",
    "\n",
    "class MY_LSTM:\n",
    "    def __init__(self, units, recurrent_activation=sigmoid, activation=np.tanh, use_bias=True):\n",
    "        self.units = units\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 1\n",
    "        self.USE_OPTIMIZER = True\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.global_step = 0\n",
    "        \n",
    "\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "#         input_dim = input_shape[-1]\n",
    "        input_dim = input_shape\n",
    "        # self.kernel is used for a new information passed to lstm, thus it has shape of (input.dim, self.units * 4)\n",
    "        # multiplication by 4 because we have 4 gates (forget, input, candidate, output)\n",
    "        self.kernel = self.__init_orthogonal(np.empty(shape=(input_dim, self.units * 4)))\n",
    "        \n",
    "        # self.recurrent_kernel is used for previous state passed to lstm, thus it has shape of (self.units, self.units * 4)\n",
    "        # multiplication by 4 because we have 4 gates (forget, input, candidate, output)\n",
    "        self.recurrent_kernel = self.__init_orthogonal(np.empty(shape=(self.units, self.units * 4)))\n",
    "        \n",
    "          # Classifier weights and biases.\n",
    "        self.classifier_kernel = self.__init_orthogonal(np.empty(shape=(self.units, input_dim)))\n",
    "\n",
    "        \n",
    "        if self.use_bias:\n",
    "            # Bias initialization which are self.units*4 in the end of all concatination\n",
    "            self.bias = np.random.uniform(low=-0.1, high=0.1, size=(self.units * 4,))\n",
    "            self.classifier_bias = np.random.uniform(low=-0.1, high=0.1,size=(input_dim,))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "    def get_weights(self):\n",
    "        return (self.kernel, self.recurrent_kernel, self.classifier_kernel)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.kernel, self.recurrent_kernel, self.classification_kernel = weights\n",
    "\n",
    "    @staticmethod\n",
    "    def __init_orthogonal(param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "        This is a common initiailization for recurrent neural networks.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "            https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        h_tm1 = np.zeros((self.batch_size, self.units))\n",
    "        c_tm1 = np.zeros((self.batch_size, self.units))\n",
    "        states = (h_tm1, c_tm1)\n",
    "        forward_cache = {\n",
    "            \"i\": [],\n",
    "            \"f\": [],\n",
    "            \"c\": [],\n",
    "            \"o\": [],\n",
    "            \"ct\": [c_tm1],\n",
    "            \"ht\": [h_tm1],\n",
    "            \"x\": []\n",
    "        }\n",
    "        for word in sentence:\n",
    "            word = np.array(word).reshape(1,1)\n",
    "#             print(\"word\", word)\n",
    "            forward_pass, states = self._step(word_input=word, states=states)\n",
    "            for key, output in forward_pass.items():\n",
    "                forward_cache[key].append(output)\n",
    "        prediction = self._apply_output_layer(states[0])\n",
    "        return forward_cache, prediction\n",
    "\n",
    "    def _step(self, word_input, states):\n",
    "        \"\"\"\n",
    "        word_input: [[9]] shape: (1, input_dim)\n",
    "        self.kernel:  shape: (input_dim, self.units*4)\n",
    "        z:            shape: (input_dim, self.units*4) \n",
    "        \"\"\"\n",
    "        h_tm1, c_tm1 = states  # stm, ltm\n",
    "        \n",
    "        # apply weights for inputs\n",
    "        concat = np.dot(word_input, self.kernel)\n",
    "        # apply weights for stm\n",
    "        concat += np.dot(h_tm1, self.recurrent_kernel)\n",
    "        \n",
    "        # apply bias\n",
    "        if self.use_bias:\n",
    "            concat += self.bias\n",
    "        concat = np.split(concat, 4, axis=1)\n",
    "        i, f, c, o = self._apply_activations(concat, c_tm1) # candidate (new ltm), output gate \n",
    "        \n",
    "        ct = f * c_tm1 + i * c     # new ltm value\n",
    "        h = o * self.activation(c) # new stm value\n",
    "                \n",
    "        forward_pass = {\n",
    "            \"i\": i,\n",
    "            \"f\": f,\n",
    "            \"c\": c,\n",
    "            \"o\": o,\n",
    "            \"ct\": ct,\n",
    "            \"ht\": h,\n",
    "            \"x\": word_input\n",
    "        }\n",
    "        # 1st return is when CURRENT lstm is final layer, 2nd return when it is feeded to next layer\n",
    "        return forward_pass, (h, c) \n",
    "    \n",
    "    def _apply_activations(self, concat, c_tm1):\n",
    "        concat0, concat1, concat2, concat3 = concat\n",
    "        i = self.recurrent_activation(concat0)\n",
    "        f = self.recurrent_activation(concat1)\n",
    "        c = self.activation(concat2)\n",
    "        o = self.recurrent_activation(concat3)\n",
    "        return i, f, c, o\n",
    "    \n",
    "    def _apply_output_layer(self, h):\n",
    "        prediction = sigmoid(np.dot(h, self.classifier_kernel))\n",
    "        if self.use_bias:\n",
    "            prediction += self.classifier_bias\n",
    "        return prediction\n",
    "# EVERYTHING UPPER IS FINE\n",
    "\n",
    "    \n",
    "\n",
    "    def compute_gradient(self, loss, prediction, forward_cache):\n",
    "        \"\"\"\n",
    "        dh:              is a gradient of classifier kernel\n",
    "        forward_cache:   is results from forward_pass\n",
    "        kernels:         are weights of LSTM\n",
    "        \n",
    "        returns          tuple(grad_kernel, grad_recurrent_kernel, grad_classification_kernel)\n",
    "        \"\"\"\n",
    "        grad_kernel, grad_recurrent_kernel, grad_classifier_kernel = self.__default_grads()\n",
    "        ltm = forward_cache[\"ct\"]\n",
    "        forget_gate, output_gate = forward_cache[\"f\"], forward_cache[\"o\"]\n",
    "        candidate_gate, input_gate = forward_cache[\"c\"], forward_cache[\"i\"]\n",
    "        \n",
    "        dh = loss * tanh_gradient(prediction)\n",
    "        for t in reversed(range(len(output_gate)-1)):\n",
    "\n",
    "            # dh is gradient of classifier kernel\n",
    "            dC = loss * output_gate[t] * tanh_gradient(ltm[t])                      \n",
    "\n",
    "            d_output_gate = loss * np.tanh(ltm[t]) * sigmoid_gradient(output_gate[t]) \n",
    "\n",
    "            d_candidate_gate = dC * input_gate[t] * tanh_gradient(candidate_gate[t]) \n",
    "\n",
    "            d_input_gate = dC * candidate_gate[t] * sigmoid_gradient(input_gate[t]) \n",
    "\n",
    "            d_forget_gate = dC * ltm[t-1] * sigmoid_gradient(forget_gate[t])    \n",
    "\n",
    "            d_gates = np.array([d_input_gate, d_forget_gate, d_candidate_gate, d_output_gate]) #Looks reasonable\n",
    "            d_kernel = (d_gates * forward_cache[\"x\"][t]).reshape(self.units*4,1)\n",
    "#             for i, gate in enumerate(d_gates):\n",
    "#                 print(f\"GATE{i}\", gate)\n",
    "            d_recurrent_kernel = (d_gates * forward_cache[\"ht\"][t-1]).reshape(self.units*4,1)\n",
    "#             print(\"BACKWARD PROPAGATION\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            grad_kernel.append(np.dot(self.kernel, d_kernel))\n",
    "            grad_recurrent_kernel.append(np.dot(self.recurrent_kernel, d_recurrent_kernel))\n",
    "            grad_classifier_kernel.append(np.dot(self.classifier_kernel, dh))\n",
    "        \n",
    "        grad_kernel = self.__clip_gradient_norm(sum(grad_kernel))\n",
    "        grad_recurrent_kernel = self.__clip_gradient_norm(sum(grad_recurrent_kernel))\n",
    "        grad_classifier_kernel = self.__clip_gradient_norm(sum(grad_classifier_kernel))\n",
    "        return (grad_kernel, grad_recurrent_kernel, grad_classifier_kernel)\n",
    "    \n",
    "    def __default_grads(self):\n",
    "        grad_kernel = np.empty_like(self.kernel)\n",
    "        grad_recurrent_kernel = np.empty_like(self.recurrent_kernel)\n",
    "        grad_classifier_kernel = np.empty_like(self.classifier_kernel)\n",
    "        return [np.array(grad_kernel)],[np.array(grad_recurrent_kernel)], [np.array(grad_classifier_kernel)]\n",
    "    \n",
    "    def __clip_gradient_norm(self, gradient, max_norm=0.1):\n",
    "        \"\"\"\n",
    "        Clips gradients to have a maximum norm of `max_norm`.\n",
    "        This is to prevent the exploding gradients problem.\n",
    "        \"\"\"\n",
    "        # Set the maximum of the norm to be of type float\n",
    "        # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "        grad_norm = np.sum(np.power(gradient, 2))\n",
    "        total_norm = grad_norm\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        # Calculate clipping coeficient\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "        if clip_coef < 1:\n",
    "            gradient *= clip_coef\n",
    "        return gradient\n",
    "  \n",
    "    def apply_gradients(self, gradients):\n",
    "        \"\"\"\n",
    "        gradients    gradients after execution of self.compute_gradients()\n",
    "        weights      current weights of LSTM\n",
    "        prev_stm     previous hidden_state to which recurrent_kernel weights are a\n",
    "        stm          current hidden_state  to which classification_kernel weight is applied\n",
    "        \"\"\"\n",
    "        # applying optimizer\n",
    "        kernels = self.get_weights()\n",
    "        kernels = self._Adam(kernels=kernels, gradients=gradients)\n",
    "#         new_kernels = []\n",
    "#         for kernel, gradient in zip(kernels, gradients):\n",
    "#             new_kernels.append(kernel * gradient)\n",
    "#         self.set_weights(new_kernels)\n",
    "    \n",
    "    \n",
    "    def _Adam(self, kernels, gradients, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Adam optimization algorithm implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update timestep\n",
    "        self.global_step += 1\n",
    "        \n",
    "        for kernel, gradient in zip(kernels, gradients):\n",
    "            first_moment = np.zeros_like(kernel)\n",
    "            second_moment = np.zeros_like(kernel)\n",
    "\n",
    "            # Perform Adam update for each parameter\n",
    "            for i in range(len(kernel)):\n",
    "                # Update biased first moment estimate\n",
    "#                 print(f\"{i} hello gradient[i]\", gradient[i])\n",
    "\n",
    "                first_moment[i] = beta1 * first_moment[i] + (1 - beta1) * gradient[i]\n",
    "\n",
    "                # Update biased second moment estimate\n",
    "\n",
    "                second_moment[i] = beta2 * second_moment[i] + (1 - beta2) * (gradient[i] ** 2)\n",
    "\n",
    "                # Correct bias in first and second moment estimates\n",
    "                first_moment_corrected = first_moment[i] / (1 - np.power(beta1, self.global_step))\n",
    "                second_moment_corrected = second_moment[i] / (1 - np.power(beta2, self.global_step))\n",
    "\n",
    "                # Update parameter\n",
    "                kernel -= learning_rate * first_moment_corrected / (np.sqrt(second_moment_corrected) + epsilon)\n",
    "\n",
    "        return kernels\n",
    "\n",
    "# x,y,z = MY_LSTM(2).__default_grads([1,2,3], [3,2,1], [1])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "2f96681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE_ADAM (array([[-0.18267242, -0.12736445, -0.0836002 , -0.45593041,  0.05368683,\n",
      "        -0.5224818 , -0.14476134,  0.08760148,  0.40235313, -0.35608687,\n",
      "         0.370259  , -0.07259851]]), array([[-1.47609841e-01,  3.12925531e-01,  4.48008582e-01,\n",
      "        -4.46083433e-01, -1.84509125e-01,  1.53764763e-01,\n",
      "         2.58020168e-01, -3.21099252e-01, -3.50647007e-01,\n",
      "        -3.83709017e-04, -3.46758392e-01, -1.00128011e-01],\n",
      "       [ 3.91333858e-01, -7.48095589e-02, -1.13785948e-01,\n",
      "        -5.88603360e-01, -3.28411740e-02, -5.06489371e-02,\n",
      "         3.09617785e-01, -1.47774671e-01,  4.78137583e-01,\n",
      "        -2.65354521e-01,  2.45895404e-01,  3.19793869e-02],\n",
      "       [ 8.62175912e-02,  5.79504784e-01,  2.10521230e-01,\n",
      "        -6.44122732e-02,  3.94100669e-01, -2.73767529e-01,\n",
      "        -2.55924082e-01,  5.57228457e-02,  9.64775585e-02,\n",
      "         8.80289949e-02,  3.00185513e-01, -4.49708872e-01]]), array([[ 0.08034021],\n",
      "       [-0.99394046],\n",
      "       [-0.07501873]]))\n",
      "\n",
      "=========PERFORM FORWARD PASS==============\n",
      "\n",
      "===========OUTPUT============================\n",
      "BACKWARD PROPAGATION\n",
      "d_kernel:  [[-7.48152298e-06]\n",
      " [ 2.18376462e-05]\n",
      " [ 1.86663248e-05]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 5.83446122e-04]\n",
      " [ 5.54830616e-04]\n",
      " [ 5.69254906e-04]\n",
      " [-7.84310094e-06]\n",
      " [ 2.13931229e-05]\n",
      " [ 1.17950586e-05]]\n",
      "d_recurrent_kernel:  [[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "BACKWARD PROPAGATION\n",
      "d_kernel:  [[-3.29444796e-06]\n",
      " [ 8.60223131e-06]\n",
      " [ 4.54984753e-06]\n",
      " [-4.59330100e-06]\n",
      " [ 9.57914122e-06]\n",
      " [ 1.16567239e-05]\n",
      " [ 2.32239385e-04]\n",
      " [ 2.19303263e-04]\n",
      " [ 2.30178043e-04]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n",
      "d_recurrent_kernel:  [[ 4.81520119e-05]\n",
      " [ 2.22303784e-04]\n",
      " [ 1.66476244e-04]\n",
      " [ 6.71361902e-05]\n",
      " [ 2.47549649e-04]\n",
      " [ 4.26512665e-04]\n",
      " [-3.39443626e-03]\n",
      " [ 5.66736042e-03]\n",
      " [ 8.42207911e-03]\n",
      " [-0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n",
      "\n",
      " [[-0.02517741 -0.03299329  0.03834886 -0.1124799   0.07100967 -0.05957663\n",
      "  -0.05831235  0.10014882  0.15143357  0.00067839  0.08405168 -0.03392288]]\n",
      "\n",
      " [[ 0.14208804  0.44056164  0.25249837 -0.00513809  0.38581206  0.58308156\n",
      "   0.30409599  0.25983272  0.08069579  0.05889048  0.25040229  0.0825072 ]\n",
      " [ 0.31497799  0.18656159  0.32315172  0.34881086  0.07686202  0.03489364\n",
      "   0.14982714  0.24794787  0.58155725  0.39615313  0.05777531  0.30223798]\n",
      " [ 0.45015014  0.15590632  0.35278856  0.10226957  0.1159275   0.05279049\n",
      "   0.48027914  0.03412094  0.21266279  0.27590908  0.09861911  0.45185043]]\n",
      "\n",
      " [[ 0.53104286]\n",
      " [-0.14618806]\n",
      " [ 0.43964728]]\n",
      "gradient[i] [-0.02517741 -0.03299329  0.03834886 -0.1124799   0.07100967 -0.05957663\n",
      " -0.05831235  0.10014882  0.15143357  0.00067839  0.08405168 -0.03392288]\n",
      "gradient[i] [ 0.14208804  0.44056164  0.25249837 -0.00513809  0.38581206  0.58308156\n",
      "  0.30409599  0.25983272  0.08069579  0.05889048  0.25040229  0.0825072 ]\n",
      "gradient[i] [0.31497799 0.18656159 0.32315172 0.34881086 0.07686202 0.03489364\n",
      " 0.14982714 0.24794787 0.58155725 0.39615313 0.05777531 0.30223798]\n",
      "gradient[i] [0.45015014 0.15590632 0.35278856 0.10226957 0.1159275  0.05279049\n",
      " 0.48027914 0.03412094 0.21266279 0.27590908 0.09861911 0.45185043]\n",
      "gradient[i] [0.53104286]\n",
      "gradient[i] [-0.14618806]\n",
      "gradient[i] [0.43964728]\n",
      "AFTER_ADAM (array([[-0.17267243, -0.11736446, -0.0936002 , -0.44593041,  0.04368683,\n",
      "        -0.5124818 , -0.13476134,  0.07760148,  0.39235314, -0.36608672,\n",
      "         0.360259  , -0.06259851]]), array([[-0.17760984,  0.28292553,  0.41800858, -0.45608345, -0.21450912,\n",
      "         0.12376477,  0.22802017, -0.35109925, -0.380647  , -0.03038371,\n",
      "        -0.37675839, -0.13012801],\n",
      "       [ 0.36133386, -0.10480956, -0.14378595, -0.59860338, -0.06284117,\n",
      "        -0.08064893,  0.27961779, -0.17777467,  0.44813759, -0.29535452,\n",
      "         0.21589541,  0.00197939],\n",
      "       [ 0.05621759,  0.54950479,  0.18052123, -0.07441229,  0.36410067,\n",
      "        -0.30376752, -0.28592408,  0.02572285,  0.06647756,  0.058029  ,\n",
      "         0.27018552, -0.47970887]]), array([[ 0.07034021],\n",
      "       [-1.00394046],\n",
      "       [-0.08501873]]))\n",
      "\n",
      "=========PERFORM FORWARD PASS==============\n",
      "\n",
      "===========OUTPUT============================\n",
      "BACKWARD PROPAGATION\n",
      "d_kernel:  [[-7.84193742e-06]\n",
      " [ 2.14374726e-05]\n",
      " [ 1.82726765e-05]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 5.82472779e-04]\n",
      " [ 5.54058345e-04]\n",
      " [ 5.68459670e-04]\n",
      " [-7.83692304e-06]\n",
      " [ 2.13853096e-05]\n",
      " [ 1.17878545e-05]]\n",
      "d_recurrent_kernel:  [[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "BACKWARD PROPAGATION\n",
      "d_kernel:  [[-3.29148351e-06]\n",
      " [ 8.59813420e-06]\n",
      " [ 4.54679179e-06]\n",
      " [-4.66501599e-06]\n",
      " [ 9.31038723e-06]\n",
      " [ 1.13559548e-05]\n",
      " [ 2.32188079e-04]\n",
      " [ 2.19255174e-04]\n",
      " [ 2.30127287e-04]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n",
      "d_recurrent_kernel:  [[ 4.80433395e-05]\n",
      " [ 2.14054049e-04]\n",
      " [ 1.62136310e-04]\n",
      " [ 6.80917727e-05]\n",
      " [ 2.31785878e-04]\n",
      " [ 4.04947640e-04]\n",
      " [-3.38907690e-03]\n",
      " [ 5.45844674e-03]\n",
      " [ 8.20622340e-03]\n",
      " [-0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n",
      "\n",
      " [[-0.02524985 -0.03339729  0.03415445 -0.11029803  0.06705175 -0.05925875\n",
      "  -0.05834821  0.09662049  0.14742758 -0.00300536  0.08006593 -0.03336292]]\n",
      "\n",
      " [[0.17188608 0.45035969 0.22229641 0.02465994 0.3556101  0.59287961\n",
      "  0.27389402 0.28963075 0.05049383 0.06868853 0.28020032 0.05230523]\n",
      " [0.28456866 0.21615225 0.35274237 0.37840151 0.10645268 0.0644843\n",
      "  0.17941779 0.21753853 0.55114791 0.3657438  0.02736597 0.27182864]\n",
      " [0.41976945 0.12552563 0.38240787 0.13188887 0.14554681 0.0824098\n",
      "  0.44989845 0.00374025 0.1822821  0.30552839 0.06823843 0.48146974]]\n",
      "\n",
      " [[ 0.54606894]\n",
      " [-0.15426784]\n",
      " [ 0.45802888]]\n",
      "gradient[i] [-0.02524985 -0.03339729  0.03415445 -0.11029803  0.06705175 -0.05925875\n",
      " -0.05834821  0.09662049  0.14742758 -0.00300536  0.08006593 -0.03336292]\n",
      "gradient[i] [0.17188608 0.45035969 0.22229641 0.02465994 0.3556101  0.59287961\n",
      " 0.27389402 0.28963075 0.05049383 0.06868853 0.28020032 0.05230523]\n",
      "gradient[i] [0.28456866 0.21615225 0.35274237 0.37840151 0.10645268 0.0644843\n",
      " 0.17941779 0.21753853 0.55114791 0.3657438  0.02736597 0.27182864]\n",
      "gradient[i] [0.41976945 0.12552563 0.38240787 0.13188887 0.14554681 0.0824098\n",
      " 0.44989845 0.00374025 0.1822821  0.30552839 0.06823843 0.48146974]\n",
      "gradient[i] [0.54606894]\n",
      "gradient[i] [-0.15426784]\n",
      "gradient[i] [0.45802888]\n",
      "AFTER_ADAM (array([[-0.16523106, -0.10992309, -0.10104157, -0.43848905,  0.03624547,\n",
      "        -0.50504043, -0.12731997,  0.07016011,  0.38491177, -0.35864539,\n",
      "         0.35281764, -0.05515715]]), array([[-0.19993394,  0.26060143,  0.39568448, -0.47840755, -0.23683323,\n",
      "         0.10144067,  0.20569607, -0.37342332, -0.40297111, -0.05270781,\n",
      "        -0.39908249, -0.15245211],\n",
      "       [ 0.33900976, -0.12713366, -0.16611005, -0.62092748, -0.08516527,\n",
      "        -0.10297303,  0.25729368, -0.20009874,  0.42581348, -0.31767862,\n",
      "         0.19357131, -0.02034471],\n",
      "       [ 0.03389349,  0.52718068,  0.15819713, -0.09673639,  0.34177657,\n",
      "        -0.32609163, -0.30824818,  0.00339877,  0.04415346,  0.03570489,\n",
      "         0.24786142, -0.50203297]]), array([[ 0.06289884],\n",
      "       [-1.01138183],\n",
      "       [-0.0924601 ]]))\n",
      "\n",
      "=========PERFORM FORWARD PASS==============\n",
      "\n",
      "===========OUTPUT============================\n",
      "BACKWARD PROPAGATION\n",
      "d_kernel:  [[-8.10993119e-06]\n",
      " [ 2.11402007e-05]\n",
      " [ 1.79802333e-05]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 5.81769427e-04]\n",
      " [ 5.53483164e-04]\n",
      " [ 5.67867351e-04]\n",
      " [-7.83229569e-06]\n",
      " [ 2.13795119e-05]\n",
      " [ 1.17825035e-05]]\n",
      "d_recurrent_kernel:  [[-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "BACKWARD PROPAGATION\n",
      "d_kernel:  [[-3.28932797e-06]\n",
      " [ 8.59509315e-06]\n",
      " [ 4.54452221e-06]\n",
      " [-4.71339499e-06]\n",
      " [ 9.11533390e-06]\n",
      " [ 1.11375191e-05]\n",
      " [ 2.32153412e-04]\n",
      " [ 2.19219569e-04]\n",
      " [ 2.30089708e-04]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n",
      "d_recurrent_kernel:  [[ 4.79678899e-05]\n",
      " [ 2.08125948e-04]\n",
      " [ 1.59000280e-04]\n",
      " [ 6.87348949e-05]\n",
      " [ 2.20723322e-04]\n",
      " [ 3.89671029e-04]\n",
      " [-3.38546639e-03]\n",
      " [ 5.30829392e-03]\n",
      " [ 8.05020334e-03]\n",
      " [-0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n",
      "\n",
      " [[-0.02521916 -0.03361316  0.03111776 -0.1095207   0.06419103 -0.05893806\n",
      "  -0.05829046  0.09407909  0.14453119 -0.00268585  0.07718457 -0.03286223]]\n",
      "\n",
      " [[0.19407326 0.47254687 0.19983538 0.04684712 0.33314907 0.61506679\n",
      "  0.251433   0.31181794 0.0280328  0.09087571 0.3023875  0.02984421]\n",
      " [0.26195786 0.23818966 0.37477975 0.40043892 0.12849009 0.0865217\n",
      "  0.20145517 0.19492774 0.52853711 0.343133   0.0047552  0.24921785]\n",
      " [0.39717975 0.10293594 0.40446638 0.15394738 0.16760532 0.1044683\n",
      "  0.42730875 0.02183998 0.1596924  0.32758689 0.04564873 0.50352824]]\n",
      "\n",
      " [[ 0.5411256 ]\n",
      " [-0.15895441]\n",
      " [ 0.45312412]]\n",
      "gradient[i] [-0.02521916 -0.03361316  0.03111776 -0.1095207   0.06419103 -0.05893806\n",
      " -0.05829046  0.09407909  0.14453119 -0.00268585  0.07718457 -0.03286223]\n",
      "gradient[i] [0.19407326 0.47254687 0.19983538 0.04684712 0.33314907 0.61506679\n",
      " 0.251433   0.31181794 0.0280328  0.09087571 0.3023875  0.02984421]\n",
      "gradient[i] [0.26195786 0.23818966 0.37477975 0.40043892 0.12849009 0.0865217\n",
      " 0.20145517 0.19492774 0.52853711 0.343133   0.0047552  0.24921785]\n",
      "gradient[i] [0.39717975 0.10293594 0.40446638 0.15394738 0.16760532 0.1044683\n",
      " 0.42730875 0.02183998 0.1596924  0.32758689 0.04564873 0.50352824]\n",
      "gradient[i] [0.5411256]\n",
      "gradient[i] [-0.15895441]\n",
      "gradient[i] [0.45312412]\n",
      "AFTER_ADAM (array([[-0.15884293, -0.10353496, -0.1074297 , -0.43210091,  0.02985733,\n",
      "        -0.4986523 , -0.12093184,  0.06377198,  0.37852363, -0.35225729,\n",
      "         0.3464295 , -0.04876901]]), array([[-0.21909835,  0.24143702,  0.37652007, -0.49757196, -0.25599763,\n",
      "         0.08227626,  0.18653166, -0.39258773, -0.42213551, -0.07187222,\n",
      "        -0.41824687, -0.17161651],\n",
      "       [ 0.31984535, -0.14629807, -0.18527446, -0.64009188, -0.10432968,\n",
      "        -0.12213744,  0.23812928, -0.21926315,  0.40664908, -0.33684303,\n",
      "         0.17440693, -0.03950912],\n",
      "       [ 0.01472908,  0.50801628,  0.13903272, -0.1159008 ,  0.32261216,\n",
      "        -0.34525603, -0.32741259, -0.01576563,  0.02498906,  0.01654049,\n",
      "         0.22869703, -0.52119738]]), array([[ 0.05651071],\n",
      "       [-1.01776997],\n",
      "       [-0.09884823]]))\n"
     ]
    }
   ],
   "source": [
    "### \"\"\"\n",
    "# TESTING OF FORWARD PASS WITH DUMMY VALUES\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# def get_\n",
    "\n",
    "np.random.seed(1337)\n",
    "# # Example data dimensions\n",
    "input_size = 1  # Number of features in the input\n",
    "units = 3  # Number of units in the hidden state/memory cell\n",
    "x = np.array([[0.002], [0.005], [0.2]])\n",
    "y = np.array([[1.0]])\n",
    "\n",
    "model = build_model(units=units, input_dim=1)\n",
    "print(\"BEFORE_ADAM\", model.get_weights())\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"\\n=========PERFORM FORWARD PASS==============\\n\")\n",
    "    forward_cache, prediction = model.forward(sentence=x)\n",
    "    print(\"===========OUTPUT============================\")\n",
    "    # print(\"prediction: \",prediction)\n",
    "    loss = bce(y, prediction).numpy() \n",
    "    # print(\"loss: \", loss)\n",
    "    dh = loss * sigmoid_gradient(prediction)\n",
    "    gradients = model.compute_gradient(loss=loss, prediction=prediction, forward_cache=forward_cache)\n",
    "    # print(\"gradients\", gradients)\n",
    "    for grad in gradients:\n",
    "        print(\"\\n\",grad)\n",
    "    model.apply_gradients(gradients)\n",
    "    print(\"AFTER_ADAM\", model.get_weights())\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n======START OF BACKWARDPROPAGATION=======\\n\")\n",
    "# dh = loss * sigmoid_gradient(prediction)\n",
    "# print(\"Loss: \", loss)\n",
    "# kernels = model.get_weights()\n",
    "# gradients = model.optimizer.compute_gradient(dh, forward_cache, kernels)\n",
    "# weights = apply_gradients(gradients, kernels, x, forward_cache[\"h_tm1\"], forward_cache[\"ht\"], lr=0.01)\n",
    "# model.set_weights(weights)\n",
    "\n",
    "# # print(f\"softmax(Output)\\n {output_softmax}\")\n",
    "# print(\"\\n\\n======FINISH OF BACKWARDPROPAGATION======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87b69e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X):\n",
    "    model = Sequential()\n",
    "    print(X.shape[1])\n",
    "    model.add(Embedding(num_words, embed_dim,input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb4b09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(X, Y, test_size=0.8):\n",
    "    length = int(len(X) * test_size)\n",
    "    X_train = X[1:length]\n",
    "    Y_train = Y[1:length]\n",
    "\n",
    "    X_valid = X[length:]\n",
    "    Y_valid = Y[length:]\n",
    "    return X_train, X_valid, Y_train, Y_valid\n",
    "\n",
    "\n",
    "def my_train(model, X, Y, hidden_size, input_size, num_epochs=2):\n",
    "    X_train, X_valid, Y_train, Y_valid = train_split(X = X, Y = Y)\n",
    "    training_loss, validation_loss = [], []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        epoch_training_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "        \n",
    "        sentence_size = len(X_train)\n",
    "        it = 0\n",
    "        for sentence, targets in zip(X_train, Y_train):\n",
    "#             print(f\"{it} sentence\", sentence)\n",
    "            forward_cache, prediction = model.forward(sentence=sentence)\n",
    "            y = np.array([targets]).reshape(1,1)\n",
    "            loss = bce(y, prediction).numpy() \n",
    "            print(f\"{it} loss: \", loss)\n",
    "            dh = loss * sigmoid_gradient(prediction)\n",
    "            print(f\"{it} train prediction: \", dh)\n",
    "            print(f\"{it} train target: \", targets)\n",
    "\n",
    "            gradients = model.compute_gradient(loss=loss, prediction=prediction, forward_cache=forward_cache)\n",
    "\n",
    "            model.apply_gradients(gradients)\n",
    "\n",
    "            epoch_training_loss += loss\n",
    "            it += 1\n",
    "\n",
    "        sentence_size = len(X_valid) \n",
    "        it = 0\n",
    "        for sentence, targets in zip(X_valid, Y_valid):            \n",
    "            forward_cache, prediction = model.forward(sentence=sentence)\n",
    "        \n",
    "            loss = bce(y, prediction).numpy() \n",
    "            # print(\"loss: \", loss)\n",
    "            dh = loss * sigmoid_gradient(prediction)\n",
    "#             print(f\"{it} valid prediction: \", dh)\n",
    "            epoch_training_loss += loss\n",
    "            it += 1\n",
    "#             print(f\"Epoch {i}, {it} out of {sentence_size} loss: \", loss)\n",
    "\n",
    "\n",
    "\n",
    "        # Save loss for plot\n",
    "        training_loss.append(epoch_training_loss / len(X_train))\n",
    "        validation_loss.append(epoch_validation_loss / len(X_valid))\n",
    "\n",
    "        # Print loss every 2 epochs\n",
    "        import statistics\n",
    "        print(f'Epoch {i+1}, training loss: {statistics.mean(training_loss)}, validation loss: {statistics.mean(validation_loss)}')\n",
    "        print(f'sentence sentence {i}:')\n",
    "        print(sentence)\n",
    "\n",
    "        print(f'\\nTarget sequence {i}:')\n",
    "        print(targets)\n",
    "\n",
    "        print('\\nPredicted sequence:')\n",
    "        output_softmax = np.dot(forward_pass[\"next_stm\"], model.parameters[\"weights\"][\"OutputSoftmax\"]) + model.parameters[\"bias\"][\"OutputSoftmax\"]\n",
    "        output_softmax = softmax(output_softmax.reshape(1,1))\n",
    "\n",
    "        print(output_softmax)\n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fa0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================FINAL=====================\n",
    "Preparing the dataset\n",
    "\"\"\"\n",
    "\n",
    "def convert(x):\n",
    "    \"\"\"\n",
    "    Coverting JSON to pandas dataframe\n",
    "\n",
    "    \"\"\"    \n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "\n",
    "\n",
    "def filter_data(data):\n",
    "    \"\"\"\n",
    "    Converting into pandas dataframe and filtering only text and ratings given by the users\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame([convert(line) for line in data])\n",
    "    df.drop(columns=df.columns.difference(['text','stars']),inplace=True)\n",
    "    df.loc[:, (\"sentiment\")] = 0\n",
    "    \n",
    "\n",
    "#I have considered a rating above 3 as positive and less than or equal to 3 as negative.\n",
    "    df.loc[:,'sentiment']=['pos' if (x>3) else 'neg' for x in df.loc[:, 'stars']]\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    for idx,row in df.iterrows():\n",
    "        df.loc[:,'text']= [x for x in df.loc[:,'text']]\n",
    "    return df\n",
    "\n",
    "def min_max_normalize(tokens):\n",
    "    min_val = min(tokens)\n",
    "    max_val = max(tokens)\n",
    "    normalized_tokens = [(token - min_val) / (max_val - min_val) for token in tokens]\n",
    "    return normalized_tokens\n",
    "\n",
    "def read_data():\n",
    "    json_filename = 'review_mockup_500.json'\n",
    "    with open(json_filename,'rb') as f:\n",
    "        data = f.readlines()\n",
    "    data = filter_data(data)\n",
    "    tokenizer = Tokenizer(num_words = 2500, split=' ')\n",
    "    tokenizer.fit_on_texts(data.loc[:,'text'].values)\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(data.loc[:,'text'].values)\n",
    "    X = [min_max_normalize(i) for i in X]\n",
    "    test = pad_sequences(X)\n",
    "\n",
    "    Y = pd.get_dummies(data['sentiment'], dtype=int).values[:, 0]   \n",
    "    return X, Y\n",
    "# print(X)\n",
    "X, Y = read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.0102429751623614\n",
      "0 train prediction:  [[0.23391113]]\n",
      "0 train target:  0\n",
      "1 loss:  0.46711230737876025\n",
      "1 train prediction:  [[0.10926659]]\n",
      "1 train target:  1\n",
      "2 loss:  0.9848288702654138\n",
      "2 train prediction:  [[0.23044852]]\n",
      "2 train target:  0\n",
      "3 loss:  1.0012682491746698\n",
      "3 train prediction:  [[0.2327155]]\n",
      "3 train target:  0\n",
      "4 loss:  0.4695390800713692\n",
      "4 train prediction:  [[0.11001409]]\n",
      "4 train target:  1\n",
      "5 loss:  0.9880663183669564\n",
      "5 train prediction:  [[0.23090286]]\n",
      "5 train target:  0\n",
      "6 loss:  1.0015816217800504\n",
      "6 train prediction:  [[0.23275775]]\n",
      "6 train target:  0\n",
      "7 loss:  0.46861976982103964\n",
      "7 train prediction:  [[0.10973101]]\n",
      "7 train target:  1\n",
      "8 loss:  0.46899717425002957\n",
      "8 train prediction:  [[0.10984724]]\n",
      "8 train target:  1\n",
      "9 loss:  0.9841107087284695\n",
      "9 train prediction:  [[0.23034721]]\n",
      "9 train target:  0\n",
      "10 loss:  0.9819662366900095\n",
      "10 train prediction:  [[0.23004355]]\n",
      "10 train target:  0\n",
      "11 loss:  0.9861604002683514\n",
      "11 train prediction:  [[0.23063586]]\n",
      "11 train target:  0\n",
      "12 loss:  0.9816431495505822\n",
      "12 train prediction:  [[0.22999765]]\n",
      "12 train target:  0\n",
      "13 loss:  0.990876398258562\n",
      "13 train prediction:  [[0.23129408]]\n",
      "13 train target:  0\n",
      "14 loss:  0.9822838935375741\n",
      "14 train prediction:  [[0.23008864]]\n",
      "14 train target:  0\n",
      "15 loss:  1.011616199139564\n",
      "15 train prediction:  [[0.23409147]]\n",
      "15 train target:  0\n",
      "16 loss:  0.9830898821023264\n",
      "16 train prediction:  [[0.23020287]]\n",
      "16 train target:  0\n",
      "17 loss:  0.982566372083082\n",
      "17 train prediction:  [[0.2301287]]\n",
      "17 train target:  0\n",
      "18 loss:  0.9825430889450721\n",
      "18 train prediction:  [[0.2301254]]\n",
      "18 train target:  0\n",
      "19 loss:  0.9914334432922318\n",
      "19 train prediction:  [[0.23137128]]\n",
      "19 train target:  0\n",
      "20 loss:  0.45786002580441304\n",
      "20 train prediction:  [[0.10641019]]\n",
      "20 train target:  1\n",
      "21 loss:  0.9901208992226159\n",
      "21 train prediction:  [[0.23118918]]\n",
      "21 train target:  0\n",
      "22 loss:  0.9860813224686862\n",
      "22 train prediction:  [[0.23062475]]\n",
      "22 train target:  0\n",
      "23 loss:  1.0109593762216034\n",
      "23 train prediction:  [[0.2340053]]\n",
      "23 train target:  0\n",
      "24 loss:  0.9818537422235762\n",
      "24 train prediction:  [[0.23002757]]\n",
      "24 train target:  0\n",
      "25 loss:  0.9861610886558179\n",
      "25 train prediction:  [[0.23063595]]\n",
      "25 train target:  0\n",
      "26 loss:  0.46185823039611795\n",
      "26 train prediction:  [[0.10764574]]\n",
      "26 train target:  1\n",
      "27 loss:  0.9832384971273347\n",
      "27 train prediction:  [[0.23022391]]\n",
      "27 train target:  0\n",
      "28 loss:  0.4578725209124503\n",
      "28 train prediction:  [[0.10641405]]\n",
      "28 train target:  1\n",
      "29 loss:  0.9818778584748434\n",
      "29 train prediction:  [[0.230031]]\n",
      "29 train target:  0\n",
      "30 loss:  0.9824352174437323\n",
      "30 train prediction:  [[0.2301101]]\n",
      "30 train target:  0\n",
      "31 loss:  1.0013910008479843\n",
      "31 train prediction:  [[0.23273205]]\n",
      "31 train target:  0\n",
      "32 loss:  0.9850991327530441\n",
      "32 train prediction:  [[0.2304866]]\n",
      "32 train target:  0\n",
      "33 loss:  0.9829560417389205\n",
      "33 train prediction:  [[0.23018392]]\n",
      "33 train target:  0\n",
      "34 loss:  1.0026583454307925\n",
      "34 train prediction:  [[0.23290263]]\n",
      "34 train target:  0\n",
      "35 loss:  0.9904717347227883\n",
      "35 train prediction:  [[0.23123792]]\n",
      "35 train target:  0\n",
      "36 loss:  0.988250370312274\n",
      "36 train prediction:  [[0.23092857]]\n",
      "36 train target:  0\n",
      "37 loss:  0.984108673680417\n",
      "37 train prediction:  [[0.23034692]]\n",
      "37 train target:  0\n",
      "38 loss:  0.9849853556920433\n",
      "38 train prediction:  [[0.23047057]]\n",
      "38 train target:  0\n",
      "39 loss:  0.9833213093156559\n",
      "39 train prediction:  [[0.23023563]]\n",
      "39 train target:  0\n",
      "40 loss:  0.46866765723536435\n",
      "40 train prediction:  [[0.10974576]]\n",
      "40 train target:  1\n",
      "41 loss:  0.9835119984862555\n",
      "41 train prediction:  [[0.2302626]]\n",
      "41 train target:  0\n",
      "42 loss:  0.9835287545708509\n",
      "42 train prediction:  [[0.23026497]]\n",
      "42 train target:  0\n",
      "43 loss:  0.4678415239711606\n",
      "43 train prediction:  [[0.10949128]]\n",
      "43 train target:  1\n",
      "44 loss:  0.986916359223367\n",
      "44 train prediction:  [[0.23074192]]\n",
      "44 train target:  0\n",
      "45 loss:  0.46724101352337066\n",
      "45 train prediction:  [[0.10930625]]\n",
      "45 train target:  1\n",
      "46 loss:  0.4643476410640685\n",
      "46 train prediction:  [[0.10841411]]\n",
      "46 train target:  1\n",
      "47 loss:  0.9865781860224861\n",
      "47 train prediction:  [[0.2306945]]\n",
      "47 train target:  0\n",
      "48 loss:  0.9840063290030479\n",
      "48 train prediction:  [[0.23033247]]\n",
      "48 train target:  0\n",
      "49 loss:  0.46826497703236525\n",
      "49 train prediction:  [[0.10962173]]\n",
      "49 train target:  1\n",
      "50 loss:  0.9836588696859402\n",
      "50 train prediction:  [[0.23028337]]\n",
      "50 train target:  0\n",
      "51 loss:  0.9904711543057528\n",
      "51 train prediction:  [[0.23123784]]\n",
      "51 train target:  0\n",
      "52 loss:  0.46096961964973326\n",
      "52 train prediction:  [[0.10737129]]\n",
      "52 train target:  1\n",
      "53 loss:  0.9842490853809789\n",
      "53 train prediction:  [[0.23036675]]\n",
      "53 train target:  0\n",
      "54 loss:  0.9750188542450293\n",
      "54 train prediction:  [[0.22904805]]\n",
      "54 train target:  0\n",
      "55 loss:  0.984766655491913\n",
      "55 train prediction:  [[0.23043975]]\n",
      "55 train target:  0\n",
      "56 loss:  0.9713193806344887\n",
      "56 train prediction:  [[0.22851062]]\n",
      "56 train target:  0\n",
      "57 loss:  0.9843017090902433\n",
      "57 train prediction:  [[0.23037417]]\n",
      "57 train target:  0\n",
      "58 loss:  0.9898610681793891\n",
      "58 train prediction:  [[0.23115306]]\n",
      "58 train target:  0\n",
      "59 loss:  0.9981339252188139\n",
      "59 train prediction:  [[0.23229097]]\n",
      "59 train target:  0\n"
     ]
    }
   ],
   "source": [
    "def build_model(units, input_dim):\n",
    "    lstm = MY_LSTM(units=units)\n",
    "    lstm.build(input_dim)\n",
    "    return lstm\n",
    "\n",
    "np.random.seed(1337)\n",
    "batch_size = 1  # Number of training examples\n",
    "input_dim = 1  # Number of features in the input\n",
    "units = 4  # Number of units in the hidden state/memory cell\n",
    "model = build_model(units=units, input_dim=input_dim)\n",
    "\n",
    "my_train(model=model, X=X, Y=Y, hidden_size=units, input_size=input_dim, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cef416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
