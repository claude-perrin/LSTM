{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2894677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#  import matplotlib.pyplot as plt\n",
    "from activation_functions import tanh_activation, sigmoid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# HyperParameters:\n",
    "embed_dim = 14\n",
    "lstm_out = 50\n",
    "batch_size = 32\n",
    "num_words = 2500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600d97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing dataset\n",
    "\n",
    "def convert(x):\n",
    "    \"\"\"\n",
    "    Coverting JSON to pandas dataframe\n",
    "\n",
    "    \"\"\"    \n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "\n",
    "\n",
    "def filter_data(data):\n",
    "    \"\"\"\n",
    "    Converting into pandas dataframe and filtering only text and ratings given by the users\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame([convert(line) for line in data])\n",
    "    df.drop(columns=df.columns.difference(['text','stars']),inplace=True)\n",
    "    df.loc[:, (\"sentiment\")] = 0\n",
    "    \n",
    "\n",
    "#     #I have considered a rating above 3 as positive and less than or equal to 3 as negative.\n",
    "    df.loc[:,'sentiment']=['pos' if (x>3) else 'neg' for x in df.loc[:, 'stars']]\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    for idx,row in df.iterrows():\n",
    "        df.loc[:,'text']= [x for x in df.loc[:,'text']]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "eb484233",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = 'review_mockup.json'\n",
    "with open(json_filename,'rb') as f:\n",
    "    data = f.readlines()\n",
    "data = filter_data(data)\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "tokenizer.fit_on_texts(data.loc[:,'text'].values)\n",
    "#print(tokenizer.word_index)  # To see the dicstionary\n",
    "X = tokenizer.texts_to_sequences(data.loc[:,'text'].values)\n",
    "# X = pad_sequences(X)\n",
    "# print((X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "50414f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_LSTM:\n",
    "    def __init__(self, hidden_size, input_size, optimizer, loss_func):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = 2\n",
    "        self._optimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self.USE_OPTIMIZER = True\n",
    "\n",
    "\n",
    "        # input_size = 2\n",
    "        # hidden_size = 1\n",
    "        # z_size = 24\n",
    "        z_size = self.hidden_size + self.input_size\n",
    "\n",
    "        self.parameters = {\n",
    "            \"weights\": {\n",
    "                \"Forget\": self.__init_orthogonal(np.random.randn(z_size, self.hidden_size)),\n",
    "                \"Input\": self.__init_orthogonal(np.random.randn(z_size, self.hidden_size)),\n",
    "                \"Candidate\": self.__init_orthogonal(np.random.randn(z_size, self.hidden_size)),\n",
    "                \"Output\": self.__init_orthogonal(np.random.randn(z_size, self.hidden_size)),\n",
    "                \"OutputSoftmax\": self.__init_orthogonal(np.random.randn(self.hidden_size, self.output_size)),\n",
    "            },\n",
    "            \"recurrent\": {\n",
    "                \"Forget\": self.__init_orthogonal(np.random.randn(self.hidden_size, self.hidden_size)),\n",
    "                \"Input\": self.__init_orthogonal(np.random.randn(self.hidden_size, self.hidden_size)),\n",
    "                \"Candidate\": self.__init_orthogonal(np.random.randn(self.hidden_size, self.hidden_size)),\n",
    "                \"Output\": self.__init_orthogonal(np.random.randn(self.hidden_size, self.hidden_size))\n",
    "            },\n",
    "            \"bias\": {\n",
    "                \"Forget\": np.ones((1, self.hidden_size)),\n",
    "                \"Input\": np.ones((1, self.hidden_size)),\n",
    "                \"Candidate\": np.ones((1, self.hidden_size)),\n",
    "                \"Output\": np.ones((1, self.hidden_size)),\n",
    "                \"OutputSoftmax\": np.ones((1, self.output_size)),\n",
    "\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, optimizer):\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "\n",
    "    @property\n",
    "    def loss_func(self):\n",
    "        return self._loss_func\n",
    "\n",
    "    @loss_func.setter\n",
    "    def loss_func(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def __init_orthogonal(self, param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "        This is a common initiailization for recurrent neural networks.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "            https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "\n",
    "    @property\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns weights and biases as 2d array\n",
    "\n",
    "        \"\"\"\n",
    "        return self.parameters\n",
    "\n",
    "    def forward(self, input_data, prev_stm, prev_ltm):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_data -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "            prev_stm -- h at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "            prev_ltm -- c at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "        Returns:\n",
    "            outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "            \n",
    "            Weight shape:\n",
    "            All of the weights    shape (self.hidden_size, z_size)\n",
    "            Except W_OutputSoftmax      (self.hidden_size, self.output_size)\n",
    "            \n",
    "            Data shapes:\n",
    "            input_data            shape (1,1)\n",
    "            prev_stm and prev_ltm shape (1,hidden_state)\n",
    "            concat                shape is (1, z_size)\n",
    "            concat.T              shape is (z_size, 1)\n",
    "             - forget_gate        shape is (hidden_size, 1)\n",
    "             - input_gate         shape is (hidden_size, 1)\n",
    "             - candidate          shape is (hidden_size, 1)\n",
    "             - next_ltm           shape is (1, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Save a list of computations for each of the components in the LSTM\n",
    "        \n",
    "        concat = np.concatenate((prev_stm, input_data), axis=1)\n",
    "#         print(f\"Concat value is: \\n\", concat, \"\\nConcat.T value is: \\n\", concat.T)\n",
    "        \n",
    "#         Compute the forget gate\n",
    "        print(\"=======Computation of the forget gate:======\")\n",
    "#         print(f\"W_forget:\\n {self.parameters['weights']['Forget']}\\nconcat:\\n {concat.T}\\nb_Forget: {self.parameters['bias']['Forget']}\")\n",
    "        z = np.dot(prev_stm, self.parameters['recurrent']['Forget'])\n",
    "        y = np.dot(concat, self.parameters['weights']['Forget'])\n",
    "        print(f\"np.dot(stm, forget): {z}\")\n",
    "        print(f\"np.dot(concat, forget): {y}\")\n",
    "        print(f\"bias: {self.parameters['bias']['Forget']}\")\n",
    "        \n",
    "        forget_gate = sigmoid(np.dot(concat, self.parameters[\"weights\"][\"Forget\"]) + np.dot(prev_stm, self.parameters[\"recurrent\"][\"Forget\"]) + self.parameters[\"bias\"][\"Forget\"])\n",
    "        print(f\"Forget_gate value is: \\n\", forget_gate)\n",
    "        print(\"===========================================\")\n",
    "\n",
    "\n",
    "        # Compute the input gate\n",
    "        print(f\"input_gate = concate.shape is {concat.shape} dot W_Input shape is {self.parameters['weights']['Input'].shape}\")\n",
    "        input_gate = sigmoid(np.dot(concat, self.parameters[\"weights\"][\"Input\"]) + np.dot(prev_stm, self.parameters[\"recurrent\"][\"Input\"]) + self.parameters[\"bias\"][\"Input\"])\n",
    "        print(f\"Input_gate value is: \\n\", input_gate)\n",
    "\n",
    "        # Compute the candidate cell value\n",
    "        candidate = np.tanh(np.dot(concat, self.parameters[\"weights\"][\"Candidate\"]) + np.dot(prev_stm, self.parameters[\"recurrent\"][\"Candidate\"]) + self.parameters[\"bias\"][\"Candidate\"])\n",
    "        print(f\"Candidate_gate value is: \\n\", candidate)\n",
    "\n",
    "        # Compute the memory cell\n",
    "        print(f\"==============next_ltm===========\\n\")\n",
    "        print(f\"forget_gate * prev_ltm:\\n {forget_gate * prev_ltm}\")\n",
    "        print(f\"input_gate * candidate: \\n {input_gate * candidate}\")\n",
    "        print(f\"next_ltm:{forget_gate * prev_ltm + input_gate * candidate} \")\n",
    "        next_ltm = forget_gate * prev_ltm + input_gate * candidate\n",
    "        print(f\"next_ltm value is: \\n\", next_ltm)\n",
    "        print(\"==================================\")\n",
    "\n",
    "\n",
    "        # Compute the output gate\n",
    "        output_gate = sigmoid(np.dot(concat, self.parameters[\"weights\"][\"Output\"]) + np.dot(prev_stm, self.parameters[\"recurrent\"][\"Output\"]) + self.parameters[\"bias\"][\"Output\"])\n",
    "        print(f\"Output_gate value is: \\n\", output_gate)\n",
    "\n",
    "        # Compute the next hidden state\n",
    "        next_stm = output_gate * np.tanh(next_ltm)\n",
    "        print(f\"next_stm value is: \\n\", next_stm)\n",
    "\n",
    "        forward_pass = {\n",
    "            \"Forget\": forget_gate,\n",
    "            \"Input\": input_gate,\n",
    "            \"Candidate\": candidate,\n",
    "            \"Output\": output_gate,\n",
    "            \"next_ltm\": next_ltm,\n",
    "            \"next_stm\": next_stm,\n",
    "            \"Concat_Input\": concat\n",
    "        }\n",
    "        \n",
    "        return forward_pass\n",
    "\n",
    "    def __cross_entropy(self, predictions, targets, epsilon=1e-12):\n",
    "        \"\"\"\n",
    "        Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "        and predictions.\n",
    "        Input: predictions (N, k) ndarray\n",
    "               targets (N, k) ndarray\n",
    "        Returns: scalar\n",
    "        \"\"\"\n",
    "        predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "        N = predictions.shape[0]\n",
    "        ce = -np.sum(targets * np.log(predictions + 1e-9)) / N\n",
    "        return ce\n",
    "\n",
    "\n",
    "    def calculate_loss(self, prediction, targets):\n",
    "        print(f\"============IN LOSS=============\")\n",
    "        print(f\"prediction: {prediction}\")\n",
    "        print(f\"targets: {targets}\")\n",
    "\n",
    "        return self.loss_func(prediction, targets)\n",
    "\n",
    "    def backward(self, forward_pass, prediction, targets):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        forward_pass -- dictionary:\n",
    "                        \"Forget_gate\": forget_gate,\n",
    "                        \"Input_gate\": input_gate,\n",
    "                        \"Candidate_gate\": candidate,\n",
    "                        \"Output_gate\": output_gate,\n",
    "                        \"next_ltm\": next_ltm.T,\n",
    "                        \"next_stm\": next_stm.T\n",
    "                        \n",
    "        targets -- your targets as a list of size m.\n",
    "        Returns:\n",
    "        loss -- crossentropy loss for all elements in output\n",
    "        grads -- lists of gradients of every element in p\n",
    "        \n",
    "        Weight shape:\n",
    "            All of the weights    shape (self.hidden_size, z_size)\n",
    "            Except W_OutputSoftmax      (self.hidden_size, self.output_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        gradients = {\n",
    "            \"weights\": {\n",
    "                \"Forget\": np.zeros_like(self.parameters[\"weights\"][\"Forget\"]),\n",
    "                \"Input\": np.zeros_like(self.parameters[\"weights\"][\"Input\"]),\n",
    "                \"Candidate\": np.zeros_like(self.parameters[\"weights\"][\"Candidate\"]),\n",
    "                \"Output\": np.zeros_like(self.parameters[\"weights\"][\"Output\"]),\n",
    "                \"OutputSoftmax\": np.zeros_like(self.parameters[\"weights\"][\"OutputSoftmax\"]),\n",
    "            },\n",
    "            \"recurrent\": {\n",
    "                \"Forget\": np.zeros_like(self.parameters[\"recurrent\"][\"Forget\"]),\n",
    "                \"Input\": np.zeros_like(self.parameters[\"recurrent\"][\"Input\"]),\n",
    "                \"Candidate\": np.zeros_like(self.parameters[\"recurrent\"][\"Candidate\"]),\n",
    "                \"Output\": np.zeros_like(self.parameters[\"recurrent\"][\"Output\"]),\n",
    "            }\n",
    "#             \"bias\": {\n",
    "#                 \"b_Forget\": np.zeros_like(self.parameters[\"bias\"][\"Forget\"]),\n",
    "#                 \"b_Input\": np.zeros_like(self.parameters[\"bias\"][\"Input\"]),\n",
    "#                 \"b_Candidate\": np.zeros_like(self.parameters[\"bias\"][\"Candidate\"]),\n",
    "#                 \"b_Output\": np.zeros_like(self.parameters[\"bias\"][\"Output\"]),\n",
    "#                 \"b_OutputSoftmax\": np.zeros_like(self.parameters[\"bias\"][\"OutputSoftmax\"]),\n",
    "#             }\n",
    "        }\n",
    "\n",
    "        # Set the next cell and hidden state equal to zero\n",
    "        print(f\"==============PERFORMING BACKWARD===========\\n\")\n",
    "        print(f\"INPUTS:\\nforward_pass: \\n{forward_pass}\\ntargets: \\n{targets}\\nprediction: \\n{prediction}\")\n",
    "        print(f\"\\n==============END OF INPUTS=================\\n\")\n",
    "        \n",
    "        print(\"\\n===============PROCESSING------------------\\n\")\n",
    "        next_stm = np.zeros_like(forward_pass[\"next_stm\"])  # h\n",
    "        next_ltm = np.zeros_like(forward_pass[\"next_ltm\"])  # C\n",
    "#         next_ltm = forward_pass[\"next_ltm\"]\n",
    "#         next_stm = forward_pass['next_stm']\n",
    "\n",
    "        loss = 0\n",
    "        # Compute the cross entropy\n",
    "        t =1\n",
    "#         for t in reversed(range(self.hidden_size)):\n",
    "        loss += self.loss_func(prediction[0].tolist(), targets)\n",
    "        print(f\"[{t}] LOSS:  {loss}\")\n",
    "        # Get the previous hidden cell state\n",
    "\n",
    "\n",
    "\n",
    "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "        dv = np.copy(prediction)\n",
    "        dv[np.argmax(targets)] -= 1\n",
    "        # Update the gradient of the relation of the hidden-state to the output gate\n",
    "        print(f\"[{t}] dv (OUTPUT_GATE):  \\n{dv}\\n\")\n",
    "#         print(f\"[{t}] gradients[W_OutputSoftmax]:  \\n{gradients['weights']['W_OutputSoftmax']}\\n\")\n",
    "        gradients[\"weights\"][\"OutputSoftmax\"] += np.dot(next_stm.T, dv)\n",
    "        #gradients[\"bias\"][\"b_OutputSoftmax\"] += dv.T \n",
    "\n",
    "\n",
    "        # Compute the derivative of the hidden state and output gate\n",
    "        dh = np.dot(dv, self.parameters[\"weights\"][\"OutputSoftmax\"].T)\n",
    "        print(\"next_stm: \",next_stm)\n",
    "        print(f\"[{t}] dh:  \\n{dh}\\n\")\n",
    "        dh += next_stm\n",
    "        \n",
    "        \n",
    "        print(f\"dh: {dh}\")\n",
    "        print(f\"next_ltm: {next_ltm}\")\n",
    "        do = dh * tanh_activation(next_ltm)\n",
    "        print(f\"do: {do}\")\n",
    "        print(f\"w_output: {forward_pass['Output']}\")\n",
    "        do = sigmoid(forward_pass[\"Output\"], derivative=True) * do\n",
    "        print(f\"[{t}] do: \\n{do}\\n\")\n",
    "        # Update the gradients with respect to the output gate\n",
    "        # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "        print(f\"do: {do.T.shape}\")\n",
    "        print(f\"Concat_input: {forward_pass['Concat_Input'].shape}\")\n",
    "        gradients[\"weights\"][\"Output\"] += np.dot(do.T, forward_pass[\"Concat_Input\"]).T\n",
    "        gradients[\"recurrent\"][\"Output\"] += np.dot(do.T, next_stm)\n",
    "\n",
    "        #gradients[\"bias\"][\"b_Output\"] += do\n",
    "        print(f\"[{t}] gradients[Output] AFTER UPDATE:  \\n{gradients['weights']['Output']}\\n\")\n",
    "\n",
    "\n",
    "        # Compute the derivative of the cell state and candidate g\n",
    "        dC = np.copy(next_ltm)\n",
    "        dC += dh * forward_pass[\"Output\"] * tanh_activation(next_ltm, derivative=True)\n",
    "        print(f\"[{t}] dC:  \\n{dC}\\n\")\n",
    "\n",
    "        dg = dC * forward_pass[\"Input\"]\n",
    "        dg = tanh_activation(forward_pass[\"Candidate\"], derivative=True) * dg\n",
    "        print(f\"[{t}] dg:  \\n{dg}\\n\")\n",
    "\n",
    "\n",
    "        # Update the gradients with respect to the candidate\n",
    "        print(f\"[{t}] gradients[Candidate] BEFORE UPDATE:  \\n{gradients['weights']['Candidate']}\\n\")\n",
    "        # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "        gradients[\"weights\"][\"Candidate\"] += np.dot(forward_pass[\"Concat_Input\"].T, dg)\n",
    "        gradients[\"recurrent\"][\"Candidate\"] += np.dot(dg.T, next_stm)\n",
    "\n",
    "        #gradients[\"bias\"][\"b_Output\"] += dg\n",
    "        print(f\"[{t}] gradients[Candidate] AFTER UPDATE:  \\n{gradients['weights']['Candidate']}\\n\")\n",
    "\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        di = dC * forward_pass[\"Candidate\"]\n",
    "        di = sigmoid(forward_pass[\"Input\"], True) * di\n",
    "        print(f\"[{t}] di:  \\n{di}\\n\")\n",
    "\n",
    "        print(f\"[{t}] gradients[Input] BEFORE UPDATE:  \\n{gradients['weights']['Input']}\\n\")\n",
    "        # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "        gradients[\"weights\"][\"Input\"] += np.dot(forward_pass[\"Concat_Input\"].T, di)\n",
    "        gradients[\"recurrent\"][\"Input\"] += np.dot(di.T, next_stm)\n",
    "\n",
    "        #gradients[\"bias\"][\"b_Input\"] += di\n",
    "        print(f\"[{t}] gradients[Input] AFTER UPDATE:  \\n{gradients['weights']['Input']}\\n\")\n",
    "\n",
    "\n",
    "        # Compute the derivative of the forget gate and update its gradients\n",
    "        df = dC * next_ltm\n",
    "        df = sigmoid(forward_pass[\"Forget\"]) * df\n",
    "        print(f\"[{t}] df:  \\n{df}\\n\")\n",
    "\n",
    "\n",
    "        print(f\"[{t}] gradients[Forget] BEFORE UPDATE:  \\n{gradients['weights']['Forget']}\\n\")\n",
    "        # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "        gradients[\"weights\"][\"Forget\"] += np.dot(forward_pass[\"Concat_Input\"].T, df)\n",
    "        gradients[\"recurrent\"][\"Forget\"] += np.dot(df.T, next_stm)\n",
    "\n",
    "        #gradients[\"bias\"][\"b_Forget\"] += df\n",
    "        print(f\"[{t}] gradients[Forget] AFTER UPDATE:  \\n{gradients['weights']['Forget']}\\n\")\n",
    "\n",
    "\n",
    "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "        dz = (np.dot(df, self.parameters[\"weights\"][\"Forget\"].T) + np.dot(di, self.parameters[\"weights\"][\"Input\"].T) + np.dot(\n",
    "            dg, self.parameters[\"weights\"][\"Candidate\"].T) + np.dot(do, self.parameters[\"weights\"][\"Output\"].T))\n",
    "        print(f\"[{t}] dz:  \\n{dz}\\n\")\n",
    "\n",
    "        dh_prev = dz[:self.hidden_size, :]\n",
    "        print(f\"[{t}] dh_prev:  \\n{dh_prev}\\n\")\n",
    "\n",
    "        dC_prev = forward_pass[\"Forget\"] * dC\n",
    "        print(f\"[{t}] dC_prev:  \\n{dC_prev}\\n\")\n",
    "\n",
    "\n",
    "        # Clip gradients\n",
    "#         print(f\"=========\\nGRADS BEFORE CLIP:  \\n{grads}\\n\")\n",
    "\n",
    "        grads = self.__clip_gradient_norm(gradients)\n",
    "\n",
    "        print(f\"=========\\nGRADS AFTER CLIP:  \\n{grads}\\n\")\n",
    "\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def __clip_gradient_norm(self, grads, max_norm=0.25):\n",
    "        \"\"\"\n",
    "        Clips gradients to have a maximum norm of `max_norm`.\n",
    "        This is to prevent the exploding gradients problem.\n",
    "        \"\"\"\n",
    "        # Set the maximum of the norm to be of type float\n",
    "        max_norm = float(max_norm)\n",
    "        total_norm = 0\n",
    "        # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "        for gate, grad in grads[\"weights\"].items():\n",
    "            grad_norm = np.sum(np.power(grad, 2))\n",
    "            total_norm += grad_norm\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        # Calculate clipping coeficient\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "        if clip_coef < 1:\n",
    "            for gate, grad in grads[\"weights\"].items():\n",
    "                grad *= clip_coef\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, t, lr=0.01):\n",
    "        # Take a step\n",
    "        parameters = self.get_parameters[\"weights\"]\n",
    "        if (self.USE_OPTIMIZER):\n",
    "            updated_parameters = self.optimizer(parameters=parameters, gradients=grads, learning_rate=lr, t=t)\n",
    "        else:\n",
    "            for (_, parameter), (_, grad) in zip(parameters.items(), grads.items()):\n",
    "                parameter -= lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ae19c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========Printing for MY_LSTM===============\n",
      "\n",
      "INPUTS\n",
      "\n",
      "(x) Input:\n",
      " [[9]]\n",
      "(prev_stm) {h} Previous hidden state:\n",
      " [[0. 0. 0. 0. 0. 0.]]\n",
      "(prev_ltm) {c} Previous memory cell:\n",
      " [[0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "=========PERFORM FORWARD PASS==============\n",
      "\n",
      "\n",
      "=========APPLY SOFTMAX (LAST LAYER OF LSTM)\n",
      "\n",
      "=======Computation of the forget gate:======\n",
      "np.dot(stm, forget): [[0. 0. 0. 0. 0. 0.]]\n",
      "np.dot(concat, forget): [[ 0.53805273  1.99633296  4.20145678 -4.13668419  4.09812628  4.08910046]]\n",
      "bias: [[1. 1. 1. 1. 1. 1.]]\n",
      "Forget_gate value is: \n",
      " [[0.82318147 0.95240819 0.99452164 0.04161918 0.9939289  0.99387419]]\n",
      "===========================================\n",
      "input_gate = concate.shape is (1, 7) dot W_Input shape is (7, 6)\n",
      "Input_gate value is: \n",
      " [[0.81862287 0.98570103 0.12355285 0.98403383 0.96104844 0.99768908]]\n",
      "Candidate_gate value is: \n",
      " [[ 0.92100606 -0.87574668  0.99475843  0.92899382 -0.99997627 -0.99713983]]\n",
      "==============next_ltm===========\n",
      "\n",
      "forget_gate * prev_ltm:\n",
      " [[0. 0. 0. 0. 0. 0.]]\n",
      "input_gate * candidate: \n",
      " [[ 0.75395663 -0.86322441  0.12290524  0.91416134 -0.96102564 -0.99483552]]\n",
      "next_ltm:[[ 0.75395663 -0.86322441  0.12290524  0.91416134 -0.96102564 -0.99483552]] \n",
      "next_ltm value is: \n",
      " [[ 0.75395663 -0.86322441  0.12290524  0.91416134 -0.96102564 -0.99483552]]\n",
      "==================================\n",
      "Output_gate value is: \n",
      " [[0.88658668 0.01098354 0.07584311 0.37295766 0.01520845 0.9155802 ]]\n",
      "next_stm value is: \n",
      " [[ 0.56520211 -0.00766558  0.00927486  0.26969448 -0.01132625 -0.69530686]]\n",
      "=======Computation of the forget gate:======\n",
      "np.dot(stm, forget): [[-0.18685687  0.36066889 -0.44993459  0.33860933  0.04879811 -0.62564928]]\n",
      "np.dot(concat, forget): [[ 0.99709017  2.85754909  5.25814724 -5.41527265  4.77602701  4.93282674]]\n",
      "bias: [[1. 1. 1. 1. 1. 1.]]\n",
      "Forget_gate value is: \n",
      " [[0.85939007 0.98548881 0.9970062  0.016681   0.99705538 0.99506854]]\n",
      "===========================================\n",
      "input_gate = concate.shape is (1, 7) dot W_Input shape is (7, 6)\n",
      "Input_gate value is: \n",
      " [[0.82058734 0.99396519 0.0925148  0.98587193 0.94385639 0.99892671]]\n",
      "Candidate_gate value is: \n",
      " [[ 0.934622   -0.74249196  0.9983317   0.92808597 -0.99999784 -0.9999429 ]]\n",
      "==============next_ltm===========\n",
      "\n",
      "forget_gate * prev_ltm:\n",
      " [[ 0.64794284 -0.850698    0.12253728  0.01524912 -0.95819578 -0.98992953]]\n",
      "input_gate * candidate: \n",
      " [[ 0.76693897 -0.73801116  0.09236046  0.91497391 -0.94385436 -0.99886967]]\n",
      "next_ltm:[[ 1.41488181 -1.58870916  0.21489774  0.93022303 -1.90205014 -1.9887992 ]] \n",
      "next_ltm value is: \n",
      " [[ 1.41488181 -1.58870916  0.21489774  0.93022303 -1.90205014 -1.9887992 ]]\n",
      "==================================\n",
      "Output_gate value is: \n",
      " [[0.91753004 0.00420491 0.04137391 0.37254169 0.00349818 0.90127392]]\n",
      "next_stm value is: \n",
      " [[ 0.8152496  -0.00386831  0.00875677  0.27221542 -0.0033457  -0.86813194]]\n",
      "===========OUTPUT============================\n",
      "(next_ltm) {h} Next hidden state:\n",
      " [[ 0.8152496  -0.00386831  0.00875677  0.27221542 -0.0033457  -0.86813194]]\n",
      "(next_stm) {c} Next memory cell:\n",
      " [[ 1.41488181 -1.58870916  0.21489774  0.93022303 -1.90205014 -1.9887992 ]]\n",
      "(Output_gate) RESHAPED:\n",
      " [0.91753004 0.00420491 0.04137391 0.37254169 0.00349818 0.90127392]\n",
      "softmax(Output)\n",
      " [[0.50111311 0.49888689]]\n",
      "\n",
      "========END OF PRINTING FOR MY_LSTM========\n",
      "\n",
      "\n",
      "========START OF TENSORFLOW LSTM===========\n",
      "\n",
      "1\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Prediction is:  [[0.5001586  0.49984145]]\n",
      "\n",
      "========END OF TENSORFLOW LSTM=============\n",
      "\n",
      "\n",
      "\n",
      "======START OF BACKWARDPROPAGATION=======\n",
      "\n",
      "==============PERFORMING BACKWARD===========\n",
      "\n",
      "INPUTS:\n",
      "forward_pass: \n",
      "{'Forget': array([[0.85939007, 0.98548881, 0.9970062 , 0.016681  , 0.99705538,\n",
      "        0.99506854]]), 'Input': array([[0.82058734, 0.99396519, 0.0925148 , 0.98587193, 0.94385639,\n",
      "        0.99892671]]), 'Candidate': array([[ 0.934622  , -0.74249196,  0.9983317 ,  0.92808597, -0.99999784,\n",
      "        -0.9999429 ]]), 'Output': array([[0.91753004, 0.00420491, 0.04137391, 0.37254169, 0.00349818,\n",
      "        0.90127392]]), 'next_ltm': array([[ 1.41488181, -1.58870916,  0.21489774,  0.93022303, -1.90205014,\n",
      "        -1.9887992 ]]), 'next_stm': array([[ 0.8152496 , -0.00386831,  0.00875677,  0.27221542, -0.0033457 ,\n",
      "        -0.86813194]]), 'Concat_Input': array([[ 5.65202108e-01, -7.66557938e-03,  9.27486145e-03,\n",
      "         2.69694477e-01, -1.13262466e-02, -6.95306857e-01,\n",
      "         1.10000000e+01]])}\n",
      "targets: \n",
      "[[1.0], [0.0]]\n",
      "prediction: \n",
      "[[1.41946047 1.41500804]]\n",
      "\n",
      "==============END OF INPUTS=================\n",
      "\n",
      "\n",
      "===============PROCESSING------------------\n",
      "\n",
      "[1] LOSS:  1.08909741909111\n",
      "[1] dv (OUTPUT_GATE):  \n",
      "[[0.41946047 0.41500804]]\n",
      "\n",
      "next_stm:  [[0. 0. 0. 0. 0. 0.]]\n",
      "[1] dh:  \n",
      "[[ 0.17849304 -0.38588375  0.12787657  0.22269283 -0.27570504 -0.15954606]]\n",
      "\n",
      "dh: [[ 0.17849304 -0.38588375  0.12787657  0.22269283 -0.27570504 -0.15954606]]\n",
      "next_ltm: [[0. 0. 0. 0. 0. 0.]]\n",
      "do: [[ 1.78499002e-13 -3.85896634e-13  1.27880836e-13  2.22700264e-13\n",
      "  -2.75714249e-13 -1.59551383e-13]]\n",
      "w_output: [[0.91753004 0.00420491 0.04137391 0.37254169 0.00349818 0.90127392]]\n",
      "[1] do: \n",
      "[[ 3.64090133e-14 -9.64737320e-14  3.19565312e-14  5.37871386e-14\n",
      "  -6.89283513e-14 -3.27702297e-14]]\n",
      "\n",
      "do: (6, 1)\n",
      "Concat_input: (1, 7)\n",
      "[1] gradients[Output] AFTER UPDATE:  \n",
      "[[ 2.05784510e-14 -5.45271566e-14  1.80618988e-14  3.04006041e-14\n",
      "  -3.89584495e-14 -1.85218029e-14]\n",
      " [-2.79096181e-16  7.39527051e-16 -2.44965327e-16 -4.12309581e-16\n",
      "   5.28375749e-16  2.51202797e-16]\n",
      " [ 3.37688554e-16 -8.94780498e-16  2.96392400e-16  4.98868259e-16\n",
      "  -6.39300909e-16 -3.03939340e-16]\n",
      " [ 9.81930978e-15 -2.60184327e-14  8.61849996e-15  1.45060942e-14\n",
      "  -1.85895956e-14 -8.83794994e-15]\n",
      " [-4.12377464e-16  1.09268528e-15 -3.61947554e-16 -6.09206398e-16\n",
      "   7.80699507e-16  3.71163703e-16]\n",
      " [-2.53154366e-14  6.70788473e-14 -2.22195953e-14 -3.73985663e-14\n",
      "   4.79263553e-14  2.27853654e-14]\n",
      " [ 4.00499146e-13 -1.06121105e-12  3.51521843e-13  5.91658525e-13\n",
      "  -7.58211865e-13 -3.60472526e-13]]\n",
      "\n",
      "[1] dC:  \n",
      "[[ 0.16377273 -0.00162261  0.00529075  0.08296236 -0.00096446 -0.1437947 ]]\n",
      "\n",
      "[1] dg:  \n",
      "[[ 0.06223456 -0.00097137  0.00020609  0.03824002 -0.00038231 -0.06033052]]\n",
      "\n",
      "[1] gradients[Candidate] BEFORE UPDATE:  \n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[1] gradients[Candidate] AFTER UPDATE:  \n",
      "[[ 3.51751046e-02 -5.49020409e-04  1.16481891e-04  2.16133389e-02\n",
      "  -2.16082823e-04 -3.40989344e-02]\n",
      " [-4.77063961e-04  7.44611436e-06 -1.57979096e-06 -2.93131896e-04\n",
      "   2.93063315e-06  4.62468355e-04]\n",
      " [ 5.77216924e-04 -9.00932279e-06  1.91144617e-06  3.54670872e-04\n",
      "  -3.54587893e-06 -5.59557172e-04]\n",
      " [ 1.67843171e-02 -2.61973142e-04  5.55810430e-05  1.03131217e-02\n",
      "  -1.03107089e-04 -1.62708068e-02]\n",
      " [-7.04883978e-04  1.10019769e-05 -2.33421392e-06 -4.33115879e-04\n",
      "   4.33014547e-06  6.83318297e-04]\n",
      " [-4.32721164e-02  6.75400268e-04 -1.43295038e-04 -2.65885469e-02\n",
      "   2.65823263e-04  4.19482210e-02]\n",
      " [ 6.84580162e-01 -1.06850707e-02  2.26697810e-03  4.20640201e-01\n",
      "  -4.20541788e-03 -6.63635669e-01]]\n",
      "\n",
      "[1] di:  \n",
      "[[0.03248416 0.00023753 0.00131766 0.01523693 0.00019449 0.02828416]]\n",
      "\n",
      "[1] gradients[Input] BEFORE UPDATE:  \n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[1] gradients[Input] AFTER UPDATE:  \n",
      "[[ 1.83601170e-02  1.34253821e-04  7.44744401e-04  8.61194640e-03\n",
      "   1.09924715e-04  1.59862657e-02]\n",
      " [-2.49009925e-04 -1.82082357e-06 -1.01006299e-05 -1.16799916e-04\n",
      "  -1.49085896e-06 -2.16814458e-04]\n",
      " [ 3.01286105e-04  2.20308023e-06  1.22211171e-05  1.41320438e-04\n",
      "   1.80384413e-06  2.62331647e-04]\n",
      " [ 8.76079915e-03  6.40611800e-05  3.55365716e-04  4.10931655e-03\n",
      "   5.24521902e-05  7.62808119e-03]\n",
      " [-3.67923634e-04 -2.69035070e-06 -1.49241460e-05 -1.72577256e-04\n",
      "  -2.20281279e-06 -3.20353349e-04]\n",
      " [-2.25864608e-02 -1.65157916e-04 -9.16178268e-04 -1.05943437e-02\n",
      "  -1.35228455e-04 -1.96661690e-02]\n",
      " [ 3.57325785e-01  2.61285655e-03  1.44942637e-02  1.67606258e-01\n",
      "   2.13936192e-03  3.11125738e-01]]\n",
      "\n",
      "[1] df:  \n",
      "[[ 0. -0.  0.  0. -0. -0.]]\n",
      "\n",
      "[1] gradients[Forget] BEFORE UPDATE:  \n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[1] gradients[Forget] AFTER UPDATE:  \n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[1] dz:  \n",
      "[[-0.02004767 -0.04528413 -0.01954711  0.10274219  0.00187497 -0.0333163\n",
      "   0.05886034]]\n",
      "\n",
      "[1] dh_prev:  \n",
      "[[-0.02004767 -0.04528413 -0.01954711  0.10274219  0.00187497 -0.0333163\n",
      "   0.05886034]]\n",
      "\n",
      "[1] dC_prev:  \n",
      "[[ 0.14074466 -0.00159906  0.00527491  0.0013839  -0.00096162 -0.14308558]]\n",
      "\n",
      "=========\n",
      "GRADS AFTER CLIP:  \n",
      "{'weights': {'Forget': array([[0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.]]), 'Input': array([[ 3.95247927e-03,  2.89015284e-05,  1.60325057e-04,\n",
      "         1.85393915e-03,  2.36640735e-05,  3.44144778e-03],\n",
      "       [-5.36056805e-05, -3.91978299e-07, -2.17441590e-06,\n",
      "        -2.51441343e-05, -3.20945076e-07, -4.66747926e-05],\n",
      "       [ 6.48594496e-05,  4.74268706e-07,  2.63090437e-06,\n",
      "         3.04227965e-05,  3.88323044e-07,  5.64735179e-05],\n",
      "       [ 1.88598346e-03,  1.37907883e-05,  7.65014528e-05,\n",
      "         8.84634259e-04,  1.12916598e-05,  1.64213729e-03],\n",
      "       [-7.92048621e-05, -5.79165992e-07, -3.21279966e-06,\n",
      "        -3.71516166e-05, -4.74211134e-07, -6.89641560e-05],\n",
      "       [-4.86230659e-03, -3.55544161e-05, -1.97230530e-04,\n",
      "        -2.28070027e-03, -2.91113432e-05, -4.23363998e-03],\n",
      "       [ 7.69234073e-02,  5.62483417e-04,  3.12025664e-03,\n",
      "         3.60814836e-02,  4.60551729e-04,  6.69776794e-02]]), 'Candidate': array([[ 7.57233038e-03, -1.18190521e-04,  2.50756714e-05,\n",
      "         4.65281753e-03, -4.65172895e-05, -7.34065756e-03],\n",
      "       [-1.02700076e-04,  1.60296433e-06, -3.40089937e-07,\n",
      "        -6.31040500e-05,  6.30892862e-07,  9.95580031e-05],\n",
      "       [ 1.24260533e-04, -1.93948446e-06,  4.11487103e-07,\n",
      "         7.63518701e-05, -7.63340069e-07, -1.20458825e-04],\n",
      "       [ 3.61324852e-03, -5.63963407e-05,  1.19652244e-05,\n",
      "         2.22016014e-03, -2.21964071e-05, -3.50270243e-03],\n",
      "       [-1.51744094e-04,  2.36845363e-06, -5.02498548e-07,\n",
      "        -9.32391407e-05,  9.32173265e-07,  1.47101536e-04],\n",
      "       [-9.31541684e-03,  1.45396980e-04, -3.08478790e-05,\n",
      "        -5.72385680e-03,  5.72251765e-05,  9.03041489e-03],\n",
      "       [ 1.47373184e-01, -2.30023156e-03,  4.88024339e-04,\n",
      "         9.05534358e-02, -9.05322500e-04, -1.42864352e-01]]), 'Output': array([[ 4.43003175e-15, -1.17383487e-14,  3.88828026e-15,\n",
      "         6.54449847e-15, -8.38679099e-15, -3.98728624e-15],\n",
      "       [-6.00825078e-17,  1.59201891e-16, -5.27349786e-17,\n",
      "        -8.87600592e-17,  1.13746236e-16,  5.40777516e-17],\n",
      "       [ 7.26959969e-17, -1.92624119e-16,  6.38059559e-17,\n",
      "         1.07394003e-16, -1.37625680e-16, -6.54306256e-17],\n",
      "       [ 2.11385463e-15, -5.60112529e-15,  1.85534997e-15,\n",
      "         3.12280345e-15, -4.00188035e-15, -1.90259212e-15],\n",
      "       [-8.87746728e-17,  2.35228127e-16, -7.79183599e-17,\n",
      "        -1.31147076e-16,  1.68065303e-16,  7.99023690e-17],\n",
      "       [-5.44978762e-15,  1.44404174e-14, -4.78332952e-15,\n",
      "        -8.05098671e-15,  1.03173594e-14,  4.90512584e-15],\n",
      "       [ 8.62175646e-14, -2.28452503e-13,  7.56739622e-14,\n",
      "         1.27369453e-13, -1.63224269e-13, -7.76008229e-14]]), 'OutputSoftmax': array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]])}, 'recurrent': {'Forget': array([[0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.]]), 'Input': array([[0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.]]), 'Candidate': array([[0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.]]), 'Output': array([[0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0.]])}}\n",
      "\n",
      "\n",
      "\n",
      "======FINISH OF BACKWARDPROPAGATION======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TESTING OF FORWARD PASS WITH DUMMY VALUES\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(1337)\n",
    "# Example data dimensions\n",
    "batch_size = 1  # Number of training examples\n",
    "input_size = 1  # Number of features in the input\n",
    "hidden_state = 6  # Number of units in the hidden state/memory cell\n",
    "model = my_build_model(hidden_size=hidden_state, input_size = input_size)\n",
    "n_y = 1  # Number of units in the output\n",
    "\n",
    "z_size = hidden_state + input_size\n",
    "# Generate example input data, previous hidden state, and previous memory cell\n",
    "x = np.array([[9]])\n",
    "z = np.array([[11]])\n",
    "prev_stm = np.zeros((batch_size, hidden_state))\n",
    "prev_ltm = np.zeros((batch_size, hidden_state))\n",
    "print(\"\\n=========Printing for MY_LSTM===============\\n\")\n",
    "print(\"INPUTS\\n\")\n",
    "print(\"(x) Input:\\n\", x)\n",
    "print(\"(prev_stm) {h} Previous hidden state:\\n\", prev_stm)\n",
    "print(\"(prev_ltm) {c} Previous memory cell:\\n\", prev_ltm)\n",
    "print(\"\\n=========PERFORM FORWARD PASS==============\\n\")\n",
    "\n",
    "# Perform forward pass\n",
    "print(\"\\n=========APPLY SOFTMAX (LAST LAYER OF LSTM)\\n\")\n",
    "forward_pass = model.forward(input_data=x, prev_stm=prev_stm, prev_ltm=prev_ltm)\n",
    "forward_pass = model.forward(input_data=z, prev_stm=forward_pass[\"next_stm\"], prev_ltm=forward_pass[\"next_ltm\"])\n",
    "reshaped_output = forward_pass['Output'].reshape(hidden_state)\n",
    "\n",
    "# Print the output\n",
    "print(\"===========OUTPUT============================\")\n",
    "print(\"(next_ltm) {h} Next hidden state:\\n\", forward_pass[\"next_stm\"])\n",
    "print(\"(next_stm) {c} Next memory cell:\\n\", forward_pass[\"next_ltm\"])\n",
    "print(\"(Output_gate) RESHAPED:\\n\", forward_pass['Output'].reshape(hidden_state))\n",
    "output_softmax = np.dot(forward_pass[\"next_stm\"], model.parameters[\"weights\"][\"OutputSoftmax\"]) + model.parameters[\"bias\"][\"OutputSoftmax\"]\n",
    "print(f\"softmax(Output)\\n {tf.nn.softmax(output_softmax)}\")\n",
    "print(\"\\n========END OF PRINTING FOR MY_LSTM========\\n\")\n",
    "\n",
    "print(\"\\n========START OF TENSORFLOW LSTM===========\\n\")\n",
    "model_tf = build_model(x)\n",
    "print(\"Prediction is: \", model_tf.predict(x))\n",
    "print(\"\\n========END OF TENSORFLOW LSTM=============\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n======START OF BACKWARDPROPAGATION=======\\n\")\n",
    "model.backward(forward_pass=forward_pass, prediction=output_softmax, targets=[[1.0],[0.0]])\n",
    "print(\"\\n\\n======FINISH OF BACKWARDPROPAGATION======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87b69e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X):\n",
    "    model = Sequential()\n",
    "    print(X.shape[1])\n",
    "    model.add(Embedding(num_words, embed_dim,input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2164ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_build_model(hidden_size, input_size):\n",
    "    lstm = MY_LSTM(hidden_size = hidden_size, input_size = input_size, optimizer=0, loss_func=mean_squared_error)\n",
    "    return lstm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "432ebbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_split(X, Y, test_size):\n",
    "    length = int(len(X) * test_size)\n",
    "    X_train = X[1:length]\n",
    "    X_valid = X[length:]\n",
    "    Y_train = Y[1:length]\n",
    "    Y_valid = Y[length:]\n",
    "    return X_train, X_valid, Y_train, Y_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bb4b09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets inside training loop: [False  True False ...  True False  True]\n",
      "input : [700, 3, 1417, 2, 135, 2259, 6, 32, 14, 746, 11, 878, 17, 34, 95, 598, 521, 3, 181, 377, 5, 184, 1, 545, 500, 2, 1417, 190, 1787, 8, 5, 1046, 10, 344, 4, 153, 1, 878, 693, 225, 6, 166, 363, 1621, 187, 6, 29, 488, 7, 29, 717, 11, 1, 759, 2, 52, 932, 2, 40, 2, 1973, 6, 63, 31, 318, 52, 51] \n",
      "targets: False \n",
      "=======Computation of the forget gate:======\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,6) and (4,4) not aligned: 6 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m Y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues[:,\u001b[38;5;241m0\u001b[39m]   \n\u001b[1;32m    103\u001b[0m model \u001b[38;5;241m=\u001b[39m my_build_model(hidden_size\u001b[38;5;241m=\u001b[39mlstm_out, input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mmy_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[194], line 43\u001b[0m, in \u001b[0;36mmy_train\u001b[0;34m(model, X, Y, hidden_size, input_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m     42\u001b[0m     word \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[word]])\n\u001b[0;32m---> 43\u001b[0m     forward_pass \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_stm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_ltm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     prev_stm \u001b[38;5;241m=\u001b[39m forward_pass[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_stm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m     prev_ltm \u001b[38;5;241m=\u001b[39m forward_pass[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_ltm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[168], line 142\u001b[0m, in \u001b[0;36mMY_LSTM.forward\u001b[0;34m(self, input_data, prev_stm, prev_ltm)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======Computation of the forget gate:======\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m#         print(f\"W_forget:\\n {self.parameters['weights']['Forget']}\\nconcat:\\n {concat.T}\\nb_Forget: {self.parameters['bias']['Forget']}\")\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_stm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecurrent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mForget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m         y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(concat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.dot(stm, forget): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,6) and (4,4) not aligned: 6 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "def my_train(model, X, Y, hidden_size, input_size):\n",
    "    \"\"\"\n",
    "    X are all sentences: [[9, 123, 5423, 121], [1,2,3,4]]\n",
    "    Y are all features:  [[1.0], [0.0]]\n",
    "    \n",
    "    \n",
    "    inputs is a sentence: [9, 123, 5423, 121]\n",
    "    targets is a feature : [1.0]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Hyper-parameters\n",
    "    num_epochs = 5\n",
    "    np.random.seed(1337)\n",
    "\n",
    "    # Initialize hidden state as zeros\n",
    "    # hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "    X_train, X_valid, Y_train, Y_valid = train_split(X = X, Y = Y, test_size = 0.20)\n",
    "\n",
    "    # Track loss\n",
    "    training_loss, validation_loss = [], []\n",
    "\n",
    "    # For each epoch\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # Track loss\n",
    "        epoch_training_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "        \n",
    "        # inputs are sentences \n",
    "        for inputs, targets in zip(X_valid, Y_valid):\n",
    "            print(f\"targets inside training loop: {Y_valid}\" )\n",
    "            # Forward pass\n",
    "            print(f\"input : {inputs} \")\n",
    "            print(f\"targets: {targets} \")\n",
    "          # Initialize hidden state and cell state as zeros\n",
    "            prev_stm = np.zeros((1, hidden_state))\n",
    "            prev_ltm = np.zeros((1, hidden_state))\n",
    "    \n",
    "            for word in inputs:\n",
    "                word = np.array([[word]])\n",
    "                forward_pass = model.forward(word, prev_stm, prev_ltm)\n",
    "                prev_stm = forward_pass[\"next_stm\"]\n",
    "                prev_ltm = forward_pass[\"next_ltm\"]\n",
    "            output_softmax = np.dot(forward_pass[\"next_stm\"], model.parameters[\"weights\"][\"OutputSoftmax\"]) + model.parameters[\"bias\"][\"OutputSoftmax\"]\n",
    "            print(\"output_softmax: \", output_softmax)\n",
    "            \n",
    "            loss = lstm.calculate_loss(forward_pass[\"result\"][-1], [[targets]])\n",
    "\n",
    "            # Update loss\n",
    "            epoch_validation_loss += loss\n",
    "       \n",
    "        # For each sentence in training set\n",
    "        t = 1\n",
    "        for inputs, targets in zip(X_train, Y_train):\n",
    "\n",
    "            # One-hot encode input and target sequence\n",
    "            #  inputs_one_hot = dataset.one_hot_encode_sequence(inputs)\n",
    "            #  targets_one_hot = dataset.one_hot_encode_sequence(targets)\n",
    "\n",
    "       \n",
    "\n",
    "            # Forward pass\n",
    "            forward_pass = model.forward(inputs, stm_prev, ltm_prev)\n",
    "\n",
    "            # Backward pass\n",
    "            loss, grads = model.backward(forward_pass, [[targets]])\n",
    "\n",
    "            # Update parameters\n",
    "\n",
    "            params = model.update_parameters(grads=grads[\"weights\"], t=t)\n",
    "            t += 1\n",
    "            # Update loss\n",
    "            #output_sentence = [dataset.idx_to_word[np.argmax(output)] for output in forward_pass[\"output_s\"]]\n",
    "\n",
    "            epoch_training_loss += loss\n",
    "\n",
    "        # Save loss for plot\n",
    "        training_loss.append(epoch_training_loss / len(X_train))\n",
    "        validation_loss.append(epoch_validation_loss / len(X_valid))\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "            print(f'Input sentence {i}:')\n",
    "            print(inputs)\n",
    "\n",
    "            print(f'\\nTarget sequence {i}:')\n",
    "            print(targets)\n",
    "\n",
    "            print('\\nPredicted sequence:')\n",
    "            print([np.argmax(output)] for output in forward_pass[\"result\"])\n",
    "    return training_loss, validation_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embed_dim = 12\n",
    "    lstm_out = 4\n",
    "    batch_size = 32\n",
    "    \n",
    "    Y = pd.get_dummies(data['sentiment']).values[:,0]   \n",
    "    model = my_build_model(hidden_size=lstm_out, input_size = 1)\n",
    "    my_train(model, X, Y, lstm_out, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fc473311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': {'Forget': array([[ 0.03048122, -0.03666253, -0.00893607, -0.02877448],\n",
      "       [ 0.02800223, -0.04561165, -0.00664564,  0.01617106],\n",
      "       [ 0.01399042,  0.03075436, -0.01881047, -0.00031739],\n",
      "       ...,\n",
      "       [ 0.01079983,  0.0273924 , -0.04248921,  0.03597927],\n",
      "       [-0.02439621,  0.01681   ,  0.01221506,  0.01667793],\n",
      "       [ 0.0215796 ,  0.01751008, -0.0202022 , -0.02570455]]), 'Input': array([[-0.0463762 ,  0.00683881, -0.01025448, -0.00243446],\n",
      "       [ 0.01002188,  0.04125598,  0.02091622, -0.02711964],\n",
      "       [-0.04309387, -0.00364571, -0.00200082,  0.06383244],\n",
      "       ...,\n",
      "       [ 0.00035293,  0.03881205, -0.00276471, -0.02780999],\n",
      "       [ 0.02801274,  0.01632623, -0.02110473, -0.0943948 ],\n",
      "       [-0.0072498 , -0.05295839,  0.02167491, -0.03179608]]), 'Candidate': array([[ 0.04432224,  0.03288289,  0.02359354, -0.08944912],\n",
      "       [-0.02213255, -0.0280371 ,  0.00056609, -0.0421228 ],\n",
      "       [ 0.00395328,  0.02036906,  0.00468574,  0.00371264],\n",
      "       ...,\n",
      "       [-0.00485133, -0.01592152, -0.019859  ,  0.05663671],\n",
      "       [-0.00434827,  0.03915324,  0.01290697,  0.06355223],\n",
      "       [ 0.04577013,  0.04618868,  0.02822208,  0.05065831]]), 'Output': array([[-0.01529156, -0.05016769, -0.00596831, -0.02596378],\n",
      "       [-0.02526063,  0.00283509,  0.00658858, -0.02544986],\n",
      "       [-0.02865882,  0.04954673, -0.04586721, -0.03751355],\n",
      "       ...,\n",
      "       [-0.0393425 , -0.02986746,  0.02798635, -0.00994667],\n",
      "       [-0.01273598, -0.02650737,  0.04095649,  0.02834531],\n",
      "       [ 0.01436783,  0.02481085, -0.03134234, -0.04105111]]), 'OutputSoftmax': array([[-0.94686397, -0.13134557],\n",
      "       [ 0.27836256, -0.72373909],\n",
      "       [ 0.16094672,  0.45504931],\n",
      "       [-0.00768473, -0.50187667]])}, 'recurrent': {'Forget': array([[ 0.24351857,  0.93346319, -0.23881026,  0.11097219],\n",
      "       [-0.67352794,  0.08952955,  0.00409193,  0.73370827],\n",
      "       [ 0.69773769, -0.24461889,  0.06665999,  0.6699852 ],\n",
      "       [-0.01486382, -0.2465614 , -0.96876693,  0.02184446]]), 'Input': array([[-0.49274938, -0.03856823, -0.79134445,  0.35983954],\n",
      "       [ 0.20802415,  0.69911168, -0.41194505, -0.54614108],\n",
      "       [ 0.59692715,  0.34868335, -0.06130611,  0.71995795],\n",
      "       [ 0.5979983 , -0.62303713, -0.44756764, -0.23217659]]), 'Candidate': array([[-0.68463768, -0.06207083,  0.46017498, -0.561834  ],\n",
      "       [ 0.32110395, -0.53913832,  0.73129957,  0.26725094],\n",
      "       [-0.64345989, -0.34447966, -0.20925044,  0.65077445],\n",
      "       [-0.11883965,  0.7660358 ,  0.4578801 ,  0.43521501]]), 'Output': array([[-0.08445564,  0.61388396, -0.73647873,  0.27131681],\n",
      "       [ 0.26220158,  0.77352076,  0.53608865, -0.21336568],\n",
      "       [ 0.20723959, -0.04573054, -0.3917028 , -0.89528174],\n",
      "       [-0.9387062 ,  0.1507343 ,  0.12952588, -0.28166094]])}, 'bias': {'Forget': array([[1., 1., 1., 1.]]), 'Input': array([[1., 1., 1., 1.]]), 'Candidate': array([[1., 1., 1., 1.]]), 'Output': array([[1., 1., 1., 1.]]), 'OutputSoftmax': array([[1., 1.]])}}\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba79ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2500\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, Y_train):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# One-hot encode input and target sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     forward_pass \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforward_pass[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and truth is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[63], line 192\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m forward_pass[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mltm_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ltm_prev\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Calculate output gate\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m o \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW_Output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_Output\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    193\u001b[0m forward_pass[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(o)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Calculate hidden state\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_split(X = X, Y = Y, test_size = 0.20)\n",
    "embed_dim = 12\n",
    "hidden_size = 300\n",
    "batch_size = 32\n",
    "input_size = 2500\n",
    "for inputs, targets in zip(X_train, Y_train):\n",
    "\n",
    "    # One-hot encode input and target sequence\n",
    "    #  inputs_one_hot = dataset.one_hot_encode_sequence(inputs)\n",
    "    #  targets_one_hot = dataset.one_hot_encode_sequence(targets)\n",
    "\n",
    "    # Forward pass\n",
    "    forward_pass = model.forward(inputs)\n",
    "    \n",
    "    \n",
    "    print(f\" Prediction: {forward_pass['result'][-1].squeeze()} and truth is {targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa0ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
