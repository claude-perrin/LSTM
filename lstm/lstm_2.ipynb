{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2894677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#  import matplotlib.pyplot as plt\n",
    "from activation_functions import softmax\n",
    "from tensorflow.keras import activations\n",
    "tf_sigmoid = activations.sigmoid\n",
    "def sigmoid(X):\n",
    "    return tf_sigmoid(X).numpy()\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x * (1 - sigmoid_x)\n",
    "\n",
    "def tanh_gradient(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50414f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true, epsilon=1e-10):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    num_samples = y_pred.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon)) / num_samples\n",
    "    return loss\n",
    "\n",
    "class MY_LSTM:\n",
    "    def __init__(self, units, optimizer=0, recurrent_activation=sigmoid, activation=np.tanh, use_bias=True):\n",
    "        self.units = units\n",
    "        self.output_size = 1\n",
    "#         self._optimizer = self.Adam\n",
    "        self.USE_OPTIMIZER = True\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, optimizer):\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        # self.kernel is used for a new information passed to lstm, thus it has shape of (input.dim, self.units * 4)\n",
    "        # multiplication by 4 because we have 4 gates (forget, input, candidate, output)\n",
    "        self.kernel = self.__init_orthogonal(np.empty(shape=(input_dim, self.units * 4)))\n",
    "        \n",
    "        # self.recurrent_kernel is used for previous state passed to lstm, thus it has shape of (self.units, self.units * 4)\n",
    "        # multiplication by 4 because we have 4 gates (forget, input, candidate, output)\n",
    "        self.recurrent_kernel = self.__init_orthogonal(np.empty(shape=(self.units, self.units * 4)))\n",
    "        \n",
    "          # Classifier weights and biases.\n",
    "        self.classifier_kernel = self.__init_orthogonal(np.empty(shape=(self.units, input_dim)))\n",
    "\n",
    "        \n",
    "        if self.use_bias:\n",
    "            # Bias initialization which are self.units*4 in the end of all concatination\n",
    "            self.bias = np.random.uniform(low=-0.1, high=0.1, size=(self.units * 4,))\n",
    "            self.classifier_bias = np.random.uniform(low=-0.1, high=0.1,size=(input_dim,))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "    def get_weights():\n",
    "        return (self.kernel, self.recurrent_kernel, self.classifier_kernel)\n",
    "\n",
    "    @staticmethod\n",
    "    def __init_orthogonal(param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "        This is a common initiailization for recurrent neural networks.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "            https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, inputs, states):\n",
    "        \"\"\"\n",
    "        inputs: [[9]] shape: (1, input_dim)\n",
    "        self.kernel:  shape: (input_dim, self.units*4)\n",
    "        z:            shape: (input_dim, self.units*4) \n",
    "        \"\"\"\n",
    "        h_tm1, c_tm1 = states  # stm, ltm\n",
    "        \n",
    "        # apply weights for inputs\n",
    "        concat = np.dot(inputs, self.kernel)\n",
    "        \n",
    "        # apply weights for stm\n",
    "        concat += np.dot(h_tm1, self.recurrent_kernel)\n",
    "        \n",
    "        # apply bias\n",
    "        if self.use_bias:\n",
    "            concat += self.bias\n",
    "        concat = np.split(concat, 4, axis=1)\n",
    "        c, o = self._apply_activations(concat, c_tm1) # candidate (new ltm), output gate \n",
    "        \n",
    "        h = o * self.activation(c) # new stm value\n",
    "                \n",
    "        forward_cache = {\n",
    "            \"i\": i,\n",
    "            \"f\": f,\n",
    "            \"ct\": c,\n",
    "            \"o\": o,\n",
    "            \"ht\": h,\n",
    "            \"h_tm1\": h_tm1,\n",
    "            \"c_tm1\": c_tm1,\n",
    "            \"inputs\": inputs\n",
    "        }\n",
    "        # 1st return is when CURRENT lstm is final layer, 2nd return when it is feeded to next layer\n",
    "        return forward_cache, (h, c) \n",
    "    \n",
    "    def _apply_activations(self, concat, c_tm1):\n",
    "        concat0, concat1, concat2, concat3 = concat\n",
    "        i = self.recurrent_activation(concat0)\n",
    "        f = self.recurrent_activation(concat1)\n",
    "        c = f * c_tm1 + i * self.activation(concat2)\n",
    "        o = self.recurrent_activation(concat3)\n",
    "        return c, o\n",
    "\n",
    "\n",
    "    def __clip_gradient_norm(self, grads, max_norm=0.25):\n",
    "        \"\"\"\n",
    "        Clips gradients to have a maximum norm of `max_norm`.\n",
    "        This is to prevent the exploding gradients problem.\n",
    "        \"\"\"\n",
    "        # Set the maximum of the norm to be of type float\n",
    "        max_norm = float(max_norm)\n",
    "        total_norm = 0\n",
    "        # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "        for gate, grad in grads[\"weights\"].items():\n",
    "            grad_norm = np.sum(np.power(grad, 2))\n",
    "            total_norm += grad_norm\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        # Calculate clipping coeficient\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "        if clip_coef < 1:\n",
    "            for gate, grad in grads[\"weights\"].items():\n",
    "                grad *= clip_coef\n",
    "        return grads\n",
    "    \n",
    "    def __default_grads(self, kernel, recurrent_kernel, classification_kernel):\n",
    "        grad_kernel = np.empty_like(kernel)\n",
    "        grad_recurrent_kernel = np.empty_like(recurrent_kernel)\n",
    "        grad_classification_kernel = np.empty_like(classification_kernel)\n",
    "        return np.array(grad_kernel), np.array(grad_recurrent_kernel), np.array(grad_classification_kernel)\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, dh, forward_cache, parameters):\n",
    "        kernel, recurrent_kernel, classification_kernel = parameters\n",
    "        grad_kernel, grad_recurrent_kernel, grad_classification_kernel = self.__default_grads(kernel, recurrent_kernel, classification_kernel)\n",
    "#         k_i, k_f, k_c, k_o = np.split(kernel, 4, axis=1)\n",
    "#         r_i, r_f, r_c, r_o = np.split(recurrent_kernel, 4, axis=1)\n",
    "        \n",
    "        # Backward propagation algorithm\n",
    "        # https://www.geeksforgeeks.org/lstm-derivation-of-back-propagation-through-time/\n",
    "        # https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9\n",
    "        # https://chat.openai.com/?model=text-davinci-002-render-sha\n",
    "        # Set the next cell and hidden state equal to zero\n",
    "        stm, ltm = forward_cache[\"ht\"], forward_cache[\"ct\"]\n",
    "        prev_stm, prev_ltm = forward_cache[\"h_tm1\"], forward_cache[\"c_tm1\"]\n",
    "\n",
    "        forget_gate, output_gate = forward_cache[\"f\"], forward_cache[\"o\"]\n",
    "        candidate_gate, input_gate = forward_cache[\"c\"], forward_cache[\"i\"]\n",
    "        inputs = forward_cache[\"inputs\"]\n",
    "        dh_next = np.zeros_like(h)\n",
    "        dC_next = np.zeros_like(c)\n",
    "        \n",
    "        # dh is gradient of softmax\n",
    "        # d_output_gate \n",
    "        dC = dh * output_gate * tanh_gradient(ltm)                        # Correct\n",
    "        \n",
    "        d_output_gate = dh * np.tanh(ltm) * sigmoid_gradient(output_gate) # Correct\n",
    "        \n",
    "        d_candidate_gate = dC * input_gate * tanh_gradient(candidate_gate) # Correct\n",
    "        \n",
    "        d_input_gate = dC * candidate_gate * sigmoid_gradient(input_gate) # Correct\n",
    "        \n",
    "        d_forget_gate = dC * prev_ltm * sigmoid_gradient(forget_gate)     # Correct\n",
    "        \n",
    "        d_gates = np.array(d_input_gate, d_forget_gate, d_candidate_gate, d_output_gate)\n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_kernel = np.dot(kernel.T, d_gates)\n",
    "        grad_recurrent_kernel = np.dot(recurrent_kernel.T, d_gates)\n",
    "        grad_classification_kernel = np.dot(classification_kernel.T, dh)\n",
    "        \n",
    "        return (grad_kernel, grad_recurrent_kernel, grad_classification_kernel)\n",
    "\n",
    "        \n",
    "        \n",
    "        # check with if statement whether to add [t+1]\n",
    "#             d_ltm[t] = loss * output_gate[t] * (1 - tanh(ltm[t])*tanh(ltm[t]))\n",
    "#             if t == self.units:\n",
    "#                 d_ltm[t] += ltm[t+1] * forget_gate[t+1]\n",
    "            \n",
    "#             d_candidate[t] = d_ltm[t] * input_gate[t] * (1-candidate_gate[t]*candidate_gate[t])\n",
    "#             d_input[t] = d_ltm[t] * candidate_gate[t] * input_gate[t] * (1-input_gate[t])\n",
    "#             d_forget[t] = d_ltm[t] * prev_ltm * forget_gate[t] * (1 - forget_gate[t])\n",
    "#             d_output[t] = loss * tanh(ltm[t]) * output_gate[t] * (1-output_gate[t])\n",
    "            \n",
    "#             d_gates = np.array(d_candidate[t], d_input[t], d_forget[t], d_output[t])            \n",
    "            \n",
    "        \n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def _Adam(self, parameters, gradients, learning_rate, global_step, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Adam optimization algorithm implementation.\n",
    "\n",
    "        Arguments:\n",
    "        parameters -- dictionary containing model parameters\n",
    "        gradients -- dictionary containing gradients of model parameters\n",
    "        learning_rate -- learning rate for the algorithm\n",
    "        beta1 -- exponential decay rate for the first moment estimates (default: 0.9)\n",
    "        beta2 -- exponential decay rate for the second moment estimates (default: 0.999)\n",
    "        epsilon -- small constant to prevent division by zero (default: 1e-8)\n",
    "\n",
    "        Returns:\n",
    "        parameters -- updated model parameters\n",
    "        \"\"\"\n",
    "        # Initialize the first and second moment estimates to zero\n",
    "        first_moment = {}\n",
    "        second_moment = {}\n",
    "\n",
    "        # Initialize the parameters with zeros\n",
    "        for param_name, param in parameters.items():\n",
    "            first_moment[param_name] = np.zeros_like(param)\n",
    "            second_moment[param_name] = np.zeros_like(param)\n",
    "\n",
    "        # Perform Adam update for each parameter\n",
    "        for param_name, param in parameters.items():\n",
    "            # Update first moment estimate\n",
    "            first_moment[param_name] = beta1 * first_moment[param_name] + (1 - beta1) * gradients[param_name]\n",
    "\n",
    "            # Update second moment estimate\n",
    "            second_moment[param_name] = beta2 * second_moment[param_name] + (1 - beta2) * np.square(gradients[param_name])\n",
    "\n",
    "            # Bias correction\n",
    "            first_moment_corrected = first_moment[param_name] / (1 - np.power(beta1, global_step))\n",
    "            second_moment_corrected = second_moment[param_name] / (1 - np.power(beta2, global_step))\n",
    "\n",
    "            # Update parameters\n",
    "            parameters[param_name] -= learning_rate * first_moment_corrected / (np.sqrt(second_moment_corrected) + epsilon)\n",
    "\n",
    "        return parameters\n",
    "        \n",
    "    \n",
    "def apply_gradients(gradients, weights, inputs, prev_stm, stm, lr=0.01):\n",
    "    grad_kernel, grad_recurrent_kernel, grad_classification_kernel = gradients\n",
    "    kernel, recurrent_kernel, classification_kernel = weights\n",
    "\n",
    "    grad_kernel *= inputs\n",
    "    grad_recurrent_kernel *= prev_stm\n",
    "    grad_classification_kernel *= stm\n",
    "\n",
    "    new_kernel = kernel - grad_kernel * self.lr\n",
    "    new_recurrent_kernel = recurrent_kernel - grad_recurrent_kernel * self.lr\n",
    "    classification_kernel = classification_kernel - grad_classification_kernel * self.lr\n",
    "\n",
    "    return new_kernel, new_recurrent_kernel, classification_kernel\n",
    "        \n",
    "\n",
    "# x,y,z = MY_LSTM(2).__default_grads([1,2,3], [3,2,1], [1])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f96681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========Printing for MY_LSTM===============\n",
      "\n",
      "INPUTS\n",
      "\n",
      "(x) Input:\n",
      " [[9]]\n",
      "(prev_stm) {h} Previous hidden state:\n",
      " [[0. 0. 0. 0. 0. 0.]]\n",
      "(prev_ltm) {c} Previous memory cell:\n",
      " [[0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "=========PERFORM FORWARD PASS==============\n",
      "\n",
      "===========OUTPUT============================\n",
      "states: \n",
      " [array([[ 0.09366264,  0.00137708, -0.07950929, -0.11217219,  0.35705341,\n",
      "        -0.02163748]]), array([[ 0.77131783,  0.00863679, -0.12254149, -0.66960698,  0.9640742 ,\n",
      "        -0.05081409]])]\n",
      "classifier_output:  [[0.54247906]]\n",
      "loss:  0.6116058007133529\n"
     ]
    }
   ],
   "source": [
    "### \"\"\"\n",
    "# TESTING OF FORWARD PASS WITH DUMMY VALUES\n",
    "# \"\"\"\n",
    "def build_model(units, input_dim):\n",
    "    lstm = MY_LSTM(units=units, optimizer=0)\n",
    "    lstm.build(input_dim)\n",
    "    return lstm\n",
    "\n",
    "\n",
    "\n",
    "# def get_\n",
    "\n",
    "np.random.seed(228)\n",
    "# # Example data dimensions\n",
    "input_size = 1  # Number of features in the input\n",
    "units = 6  # Number of units in the hidden state/memory cell\n",
    "x = np.array([[9]])\n",
    "y = np.array([[1.0]])\n",
    "\n",
    "model = build_model(units=units, input_dim=x.shape)\n",
    "\n",
    "prev_stm = np.zeros((1, units))\n",
    "prev_ltm = np.zeros((1, units))\n",
    "states = (prev_stm, prev_ltm)\n",
    "print(\"\\n=========Printing for MY_LSTM===============\\n\")\n",
    "print(\"INPUTS\\n\")\n",
    "print(\"(x) Input:\\n\", x)\n",
    "print(\"(prev_stm) {h} Previous hidden state:\\n\", prev_stm)\n",
    "print(\"(prev_ltm) {c} Previous memory cell:\\n\", prev_ltm)\n",
    "print(\"\\n=========PERFORM FORWARD PASS==============\\n\")\n",
    "forward_pass = model.forward(inputs=x, states=states)\n",
    "h, states = forward_pass\n",
    "\n",
    "classifier_output = sigmoid(np.dot(h, model.classifier_kernel))\n",
    "if model.use_bias:\n",
    "    classifier_output += model.classifier_bias\n",
    "print(\"===========OUTPUT============================\")\n",
    "print(\"states: \\n\", states)\n",
    "print(\"classifier_output: \",classifier_output)\n",
    "loss = cross_entropy_loss(classifier_output, y)\n",
    "print(\"loss: \", loss)\n",
    "\n",
    "\n",
    "# print(\"\\n\\n======START OF BACKWARDPROPAGATION=======\\n\")\n",
    "# print(f\"softmax(Output)\\n {output_softmax}\")\n",
    "# model.backward(forward_pass=forward_pass, prediction=output_softmax, targets=[[1.0],[0.0]])\n",
    "# print(\"\\n\\n======FINISH OF BACKWARDPROPAGATION======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b69e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X):\n",
    "    model = Sequential()\n",
    "    print(X.shape[1])\n",
    "    model.add(Embedding(num_words, embed_dim,input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(X, Y, test_size=0.8):\n",
    "    length = int(len(X) * test_size)\n",
    "    X_train = X[1:length]\n",
    "    Y_train = Y[1:length]\n",
    "\n",
    "    X_valid = X[length:]\n",
    "    Y_valid = Y[length:]\n",
    "    return X_train, X_valid, Y_train, Y_valid\n",
    "\n",
    "\n",
    "def my_train(model, X, Y, hidden_size, input_size, num_epochs=2):\n",
    "    X_train, X_valid, Y_train, Y_valid = train_split(X = X, Y = Y)\n",
    "    training_loss, validation_loss = [], []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        epoch_training_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "        \n",
    "        sentence_size = len(X_train)\n",
    "        it = 0\n",
    "        for sentence, targets in zip(X_train, Y_train):\n",
    "\n",
    "            prev_stm = np.zeros((1, hidden_size))\n",
    "            prev_ltm = np.zeros((1, hidden_size))\n",
    "            forward_cache = []\n",
    "\n",
    "       \n",
    "            for word in sentence:\n",
    "                word = np.array([[word]])\n",
    "                forward_pass, prev_stm, prev_ltm = model.forward(word, prev_stm, prev_ltm)\n",
    "                forward_cache.append(forward_pass)\n",
    "            \n",
    "            prediction = sigmoid(prev_stm) * self.classifier_kernel + bias.classifier_kernel\n",
    "            loss = # calculate loss (prediction, targets)\n",
    "            dh = loss * prediction\n",
    "            gradients = backward(dh, forward_cache) # start backward with \n",
    "            weights = model.get_weights()\n",
    "            apply_gradients(gradients, weights, inputs, prev_stm, stm, lr=0.01):\n",
    "            \n",
    "            epoch_training_loss += loss\n",
    "            it += 1\n",
    "\n",
    "        sentence_size = len(X_valid) \n",
    "        it = 0\n",
    "        for sentence, targets in zip(X_valid, Y_valid):            \n",
    "            prev_stm = np.zeros((1, hidden_size))\n",
    "            prev_ltm = np.zeros((1, hidden_size))\n",
    "    \n",
    "            for word in sentence:\n",
    "                word = np.array([[word]])\n",
    "                forward_pass = model.forward(word, prev_stm, prev_ltm)\n",
    "                prev_stm = forward_pass[\"next_stm\"]\n",
    "                prev_ltm = forward_pass[\"next_ltm\"]\n",
    "            \n",
    "            print(\"forward_pass['next_stm']\", forward_pass[\"next_stm\"])\n",
    "            output_softmax = np.dot(forward_pass[\"next_stm\"], model.parameters[\"weights\"][\"OutputSoftmax\"]) + model.parameters[\"bias\"][\"OutputSoftmax\"]\n",
    "            print(\"=========BEFORE SOFTMAX=========\\n\", output_softmax)\n",
    "            output_softmax = sigmoid(output_softmax.reshape(1,1))\n",
    "            print(\"=========AFTER SOFTMAX=========\\n\", output_softmax)\n",
    "\n",
    "            \n",
    "            loss = model.calculate_loss(output_softmax, targets)\n",
    "\n",
    "            # Update loss\n",
    "            epoch_validation_loss += loss\n",
    "            it += 1\n",
    "#             print(f\"Epoch {i}, {it} out of {sentence_size} loss: \", loss)\n",
    "\n",
    "\n",
    "\n",
    "        # Save loss for plot\n",
    "        training_loss.append(epoch_training_loss / len(X_train))\n",
    "        validation_loss.append(epoch_validation_loss / len(X_valid))\n",
    "\n",
    "        # Print loss every 2 epochs\n",
    "        import statistics\n",
    "        print(f'Epoch {i+1}, training loss: {statistics.mean(training_loss)}, validation loss: {statistics.mean(validation_loss)}')\n",
    "        print(f'sentence sentence {i}:')\n",
    "        print(sentence)\n",
    "\n",
    "        print(f'\\nTarget sequence {i}:')\n",
    "        print(targets)\n",
    "\n",
    "        print('\\nPredicted sequence:')\n",
    "        output_softmax = np.dot(forward_pass[\"next_stm\"], model.parameters[\"weights\"][\"OutputSoftmax\"]) + model.parameters[\"bias\"][\"OutputSoftmax\"]\n",
    "        output_softmax = softmax(output_softmax.reshape(1,1))\n",
    "\n",
    "        print(output_softmax)\n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================FINAL=====================\n",
    "Preparing the dataset\n",
    "\"\"\"\n",
    "\n",
    "def convert(x):\n",
    "    \"\"\"\n",
    "    Coverting JSON to pandas dataframe\n",
    "\n",
    "    \"\"\"    \n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "\n",
    "\n",
    "def filter_data(data):\n",
    "    \"\"\"\n",
    "    Converting into pandas dataframe and filtering only text and ratings given by the users\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame([convert(line) for line in data])\n",
    "    df.drop(columns=df.columns.difference(['text','stars']),inplace=True)\n",
    "    df.loc[:, (\"sentiment\")] = 0\n",
    "    \n",
    "\n",
    "#I have considered a rating above 3 as positive and less than or equal to 3 as negative.\n",
    "    df.loc[:,'sentiment']=['pos' if (x>3) else 'neg' for x in df.loc[:, 'stars']]\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    for idx,row in df.iterrows():\n",
    "        df.loc[:,'text']= [x for x in df.loc[:,'text']]\n",
    "    return df\n",
    "\n",
    "def read_data():\n",
    "    json_filename = 'review_mockup_500.json'\n",
    "    with open(json_filename,'rb') as f:\n",
    "        data = f.readlines()\n",
    "    data = filter_data(data)\n",
    "    tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "    tokenizer.fit_on_texts(data.loc[:,'text'].values)\n",
    "    X = tokenizer.texts_to_sequences(data.loc[:,'text'].values)\n",
    "    Y = pd.get_dummies(data['sentiment'], dtype=int).values[:, 0]   \n",
    "    return X, Y\n",
    "\n",
    "X, Y = read_data()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "batch_size = 1  # Number of training examples\n",
    "input_size = 1  # Number of features in the input\n",
    "hidden_size = 4  # Number of units in the hidden state/memory cell\n",
    "model = MY_LSTM.my_build_model(hidden_size=hidden_size, input_size=input_size)\n",
    "prev_stm = np.zeros((batch_size, hidden_size))\n",
    "prev_ltm = np.zeros((batch_size, hidden_size))\n",
    "\n",
    "my_train(model=model, X=X, Y=Y, hidden_size=hidden_size, input_size=input_size, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cef416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
