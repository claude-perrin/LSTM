{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2894677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#  import matplotlib.pyplot as plt\n",
    "from activation_functions import softmax\n",
    "from tensorflow.keras import activations\n",
    "tf_sigmoid = activations.sigmoid\n",
    "def sigmoid(X):\n",
    "    return tf_sigmoid(X).numpy()\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x * (1 - sigmoid_x)\n",
    "\n",
    "def tanh_gradient(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "50414f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y_true, epsilon=1e-10):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    num_samples = y_pred.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + epsilon)) / num_samples\n",
    "    return loss\n",
    "\n",
    "class MY_LSTM:\n",
    "    def __init__(self, units, recurrent_activation=sigmoid, activation=np.tanh, use_bias=True):\n",
    "        self.units = units\n",
    "        self.output_size = 1\n",
    "        self.batch_size = 1\n",
    "        self.USE_OPTIMIZER = True\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.global_step = 0\n",
    "        \n",
    "\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "#         input_dim = input_shape[-1]\n",
    "        input_dim = input_shape\n",
    "        # self.kernel is used for a new information passed to lstm, thus it has shape of (input.dim, self.units * 4)\n",
    "        # multiplication by 4 because we have 4 gates (forget, input, candidate, output)\n",
    "        self.kernel = self.__init_orthogonal(np.empty(shape=(input_dim, self.units * 4)))\n",
    "        \n",
    "        # [0.001, 0.1, 0.2, 0.5]\n",
    "        # self.recurrent_kernel is used for previous state passed to lstm, thus it has shape of (self.units, self.units * 4)\n",
    "        # multiplication by 4 because we have 4 gates (forget, input, candidate, output)\n",
    "        self.recurrent_kernel = self.__init_orthogonal(np.empty(shape=(self.units, self.units * 4)))\n",
    "        \n",
    "          # Classifier weights and biases.\n",
    "        self.classifier_kernel = self.__init_orthogonal(np.empty(shape=(self.units, input_dim)))\n",
    "\n",
    "        \n",
    "        if self.use_bias:\n",
    "            # Bias initialization which are self.units*4 in the end of all concatination\n",
    "            self.bias = np.random.uniform(low=-0.3, high=0.3, size=(self.units * 4,))\n",
    "            self.classifier_bias = np.random.uniform(low=-0.3, high=0.3,size=(input_dim,))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "    def get_weights(self):\n",
    "        return (self.kernel, self.recurrent_kernel, self.classifier_kernel)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.kernel, self.recurrent_kernel, self.classification_kernel = weights\n",
    "\n",
    "    @staticmethod\n",
    "    def __init_orthogonal(param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "        This is a common initiailization for recurrent neural networks.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "            https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        h_tm1 = np.zeros((self.batch_size, self.units))\n",
    "        c_tm1 = np.zeros((self.batch_size, self.units))\n",
    "        states = (h_tm1, c_tm1)\n",
    "        forward_cache = {\n",
    "            \"i\": [],\n",
    "            \"f\": [],\n",
    "            \"c\": [],\n",
    "            \"o\": [],\n",
    "            \"ct\": [c_tm1],\n",
    "            \"ht\": [h_tm1],\n",
    "            \"x\": []\n",
    "        }\n",
    "        for word in sentence:\n",
    "            word = np.array(word).reshape(1,1)\n",
    "#             print(\"word\", word)\n",
    "            forward_pass, states = self._step(word_input=word, states=states)\n",
    "            for key, output in forward_pass.items():\n",
    "                forward_cache[key].append(output)\n",
    "        prediction = self._apply_output_layer(states[0])\n",
    "        return forward_cache, prediction\n",
    "\n",
    "    def _step(self, word_input, states):\n",
    "        \"\"\"\n",
    "        word_input: [[9]] shape: (1, input_dim)\n",
    "        self.kernel:  shape: (input_dim, self.units*4)\n",
    "        z:            shape: (input_dim, self.units*4) \n",
    "        \"\"\"\n",
    "        h_tm1, c_tm1 = states  # stm, ltm\n",
    "        \n",
    "        # apply weights for inputs\n",
    "        concat = np.dot(word_input, self.kernel)\n",
    "        # apply weights for stm\n",
    "        concat += np.dot(h_tm1, self.recurrent_kernel)\n",
    "        \n",
    "        # apply bias\n",
    "        if self.use_bias:\n",
    "            concat += self.bias\n",
    "        concat = np.split(concat, 4, axis=1)\n",
    "        i, f, c, o = self._apply_activations(concat, c_tm1) # candidate (new ltm), output gate \n",
    "        \n",
    "        ct = f * c_tm1 + i * c     # new ltm value\n",
    "        h = o * self.activation(c) # new stm value\n",
    "                \n",
    "        forward_pass = {\n",
    "            \"i\": i,\n",
    "            \"f\": f,\n",
    "            \"c\": c,\n",
    "            \"o\": o,\n",
    "            \"ct\": ct,\n",
    "            \"ht\": h,\n",
    "            \"x\": word_input\n",
    "        }\n",
    "        # 1st return is when CURRENT lstm is final layer, 2nd return when it is feeded to next layer\n",
    "        return forward_pass, (h, c) \n",
    "    \n",
    "    def _apply_activations(self, concat, c_tm1):\n",
    "        concat0, concat1, concat2, concat3 = concat\n",
    "        i = self.recurrent_activation(concat0)\n",
    "        f = self.recurrent_activation(concat1)\n",
    "        c = self.activation(concat2)\n",
    "        o = self.recurrent_activation(concat3)\n",
    "        return i, f, c, o\n",
    "    \n",
    "    def _apply_output_layer(self, h):\n",
    "        prediction = sigmoid(np.dot(h, self.classifier_kernel))\n",
    "        if self.use_bias:\n",
    "            prediction += self.classifier_bias\n",
    "        return prediction\n",
    "# EVERYTHING UPPER IS FINE\n",
    "\n",
    "    \n",
    "\n",
    "    def compute_gradient(self, loss, prediction, forward_cache):\n",
    "        \"\"\"\n",
    "        dh:              is a gradient of classifier kernel\n",
    "        forward_cache:   is results from forward_pass\n",
    "        kernels:         are weights of LSTM\n",
    "        \n",
    "        returns          tuple(grad_kernel, grad_recurrent_kernel, grad_classification_kernel)\n",
    "        \"\"\"\n",
    "#         grad_kernel, grad_recurrent_kernel, grad_classifier_kernel = self.__default_grads()\n",
    "        grad_kernel = []\n",
    "        grad_recurrent_kernel = []\n",
    "        grad_classifier_kernel = []\n",
    "\n",
    "        ltm = forward_cache[\"ct\"]\n",
    "        forget_gate, output_gate = forward_cache[\"f\"], forward_cache[\"o\"]\n",
    "        candidate_gate, input_gate = forward_cache[\"c\"], forward_cache[\"i\"]\n",
    "        \n",
    "        dh = loss * tanh_gradient(prediction)\n",
    "        for t in reversed(range(len(output_gate)-1)):\n",
    "\n",
    "            # dh is gradient of classifier kernel\n",
    "            dC = loss * output_gate[t] * tanh_gradient(ltm[t])                      \n",
    "\n",
    "            d_output_gate = loss * np.tanh(ltm[t]) * sigmoid_gradient(output_gate[t]) \n",
    "\n",
    "            d_candidate_gate = dC * input_gate[t] * tanh_gradient(candidate_gate[t]) \n",
    "\n",
    "            d_input_gate = dC * candidate_gate[t] * sigmoid_gradient(input_gate[t]) \n",
    "\n",
    "            d_forget_gate = dC * ltm[t-1] * sigmoid_gradient(forget_gate[t])    \n",
    "\n",
    "            d_gates = np.array([d_input_gate, d_forget_gate, d_candidate_gate, d_output_gate]).reshape(1, self.units*4) #Looks reasonable\n",
    "            d_kernel = (d_gates * forward_cache[\"x\"][t]).reshape(1, self.units*4,)\n",
    "            \n",
    "            d_gates = d_gates.reshape(self.units*4, 1)\n",
    "            d_recurrent_kernel = (d_gates * forward_cache[\"ht\"][t-1]).reshape(self.units, self.units*4)\n",
    "            \n",
    "            grad_kernel.append(self.kernel * d_kernel)\n",
    "            grad_recurrent_kernel.append(self.recurrent_kernel * d_recurrent_kernel)\n",
    "            grad_classifier_kernel.append(self.classifier_kernel * dh)\n",
    "        \n",
    "        grad_kernel = self.__clip_gradient_norm(sum(grad_kernel))\n",
    "        grad_recurrent_kernel = self.__clip_gradient_norm(sum(grad_recurrent_kernel))\n",
    "        grad_classifier_kernel = self.__clip_gradient_norm(sum(grad_classifier_kernel))\n",
    "        return (grad_kernel, grad_recurrent_kernel, grad_classifier_kernel)\n",
    "    \n",
    "#     def __default_grads(self):\n",
    "#         grad_kernel = np.empty_like(self.kernel)\n",
    "#         grad_recurrent_kernel = np.empty_like(self.recurrent_kernel)\n",
    "#         grad_classifier_kernel = np.empty_like(self.classifier_kernel)\n",
    "#         return [np.array(grad_kernel)],[np.array(grad_recurrent_kernel)], [np.array(grad_classifier_kernel)]\n",
    "    \n",
    "    def __clip_gradient_norm(self, gradient, max_norm=0.1):\n",
    "        \"\"\"\n",
    "        Clips gradients to have a maximum norm of `max_norm`.\n",
    "        This is to prevent the exploding gradients problem.\n",
    "        \"\"\"\n",
    "        # Set the maximum of the norm to be of type float\n",
    "        # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "        grad_norm = np.sum(np.power(gradient, 2))\n",
    "        total_norm = grad_norm\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        # Calculate clipping coeficient\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "        if clip_coef < 1:\n",
    "            gradient *= clip_coef\n",
    "        return gradient\n",
    "  \n",
    "    def apply_gradients(self, gradients):\n",
    "        \"\"\"\n",
    "        gradients    gradients after execution of self.compute_gradients()\n",
    "        weights      current weights of LSTM\n",
    "        prev_stm     previous hidden_state to which recurrent_kernel weights are a\n",
    "        stm          current hidden_state  to which classification_kernel weight is applied\n",
    "        \"\"\"\n",
    "        # applying optimizer\n",
    "        kernels = self.get_weights()\n",
    "        \n",
    "        kernels = self._Adam(kernels=kernels, gradients=gradients)\n",
    "#         new_kernels = []\n",
    "#         for kernel, gradient in zip(kernels, gradients):\n",
    "#             new_kernels.append(kernel * gradient)\n",
    "#         self.set_weights(new_kernels)\n",
    "    \n",
    "    \n",
    "    def _Adam(self, kernels, gradients, learning_rate=1, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Adam optimization algorithm implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update timestep\n",
    "        self.global_step += 1\n",
    "        \n",
    "        for kernel, gradient in zip(kernels, gradients):\n",
    "            first_moment = np.zeros_like(kernel)\n",
    "            second_moment = np.zeros_like(kernel)\n",
    "\n",
    "            # Perform Adam update for each parameter\n",
    "            for i in range(len(kernel)):\n",
    "                # Update biased first moment estimate\n",
    "#                 print(f\"{i} hello gradient[i]\", gradient[i])\n",
    "\n",
    "                first_moment[i] = beta1 * first_moment[i] + (1 - beta1) * gradient[i]\n",
    "\n",
    "                # Update biased second moment estimate\n",
    "#                 print(f\"gradient[{i}]\", gradient[i])\n",
    "                second_moment[i] = beta2 * second_moment[i] + (1 - beta2) * (gradient[i] ** 2)\n",
    "\n",
    "                # Correct bias in first and second moment estimates\n",
    "                first_moment_corrected = first_moment[i] / (1 - np.power(beta1, self.global_step))\n",
    "                second_moment_corrected = second_moment[i] / (1 - np.power(beta2, self.global_step))\n",
    "\n",
    "                # Update parameter\n",
    "                kernel -= learning_rate * first_moment_corrected / (np.sqrt(second_moment_corrected) + epsilon)\n",
    "\n",
    "        return kernels\n",
    "\n",
    "# x,y,z = MY_LSTM(2).__default_grads([1,2,3], [3,2,1], [1])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"\"\"\n",
    "# TESTING OF FORWARD PASS WITH DUMMY VALUES\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# def get_\n",
    "\n",
    "np.random.seed(1337)\n",
    "# # Example data dimensions\n",
    "input_size = 1  # Number of features in the input\n",
    "units = 3  # Number of units in the hidden state/memory cell\n",
    "x = np.array([[0.002], [0.005], [0.2]])\n",
    "y = np.array([[1.0]])\n",
    "\n",
    "model = build_model(units=units, input_dim=1)\n",
    "print(\"BEFORE_ADAM\", model.get_weights())\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"\\n=========PERFORM FORWARD PASS==============\\n\")\n",
    "    forward_cache, prediction = model.forward(sentence=x)\n",
    "    print(\"===========OUTPUT============================\")\n",
    "    # print(\"prediction: \",prediction)\n",
    "    loss = bce(y, prediction).numpy() \n",
    "    # print(\"loss: \", loss)\n",
    "    dh = loss * sigmoid_gradient(prediction)\n",
    "    gradients = model.compute_gradient(loss=loss, prediction=prediction, forward_cache=forward_cache)\n",
    "    # print(\"gradients\", gradients)\n",
    "    for grad in gradients:\n",
    "        print(\"\\n\",grad)\n",
    "    model.apply_gradients(gradients)\n",
    "    print(\"AFTER_ADAM\", model.get_weights())\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n======START OF BACKWARDPROPAGATION=======\\n\")\n",
    "# dh = loss * sigmoid_gradient(prediction)\n",
    "# print(\"Loss: \", loss)\n",
    "# kernels = model.get_weights()\n",
    "# gradients = model.optimizer.compute_gradient(dh, forward_cache, kernels)\n",
    "# weights = apply_gradients(gradients, kernels, x, forward_cache[\"h_tm1\"], forward_cache[\"ht\"], lr=0.01)\n",
    "# model.set_weights(weights)\n",
    "\n",
    "# # print(f\"softmax(Output)\\n {output_softmax}\")\n",
    "# print(\"\\n\\n======FINISH OF BACKWARDPROPAGATION======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b69e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X):\n",
    "    model = Sequential()\n",
    "    print(X.shape[1])\n",
    "    model.add(Embedding(num_words, embed_dim,input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb4b09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(X, Y, test_size=0.8):\n",
    "    length = int(len(X) * test_size)\n",
    "    X_train = X[1:length]\n",
    "    Y_train = Y[1:length]\n",
    "\n",
    "    X_valid = X[length:]\n",
    "    Y_valid = Y[length:]\n",
    "    return X_train, X_valid, Y_train, Y_valid\n",
    "\n",
    "\n",
    "def my_train(model, X, Y, hidden_size, input_size, num_epochs=2):\n",
    "    X_train, X_valid, Y_train, Y_valid = train_split(X = X, Y = Y)\n",
    "    training_loss, validation_loss = [], []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        epoch_training_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "        \n",
    "        sentence_size = len(X_train)\n",
    "        it = 0\n",
    "        for sentence, targets in zip(X_train, Y_train):\n",
    "#             print(f\"{it} sentence\", sentence)\n",
    "            forward_cache, prediction = model.forward(sentence=sentence)\n",
    "#             print(f\"{it} h: \", forward_cache[\"ht\"][-1])\n",
    "            y = np.array([targets]).reshape(1,1)\n",
    "            loss = bce(y, prediction).numpy() \n",
    "            print(f\"{it} loss: \", loss)\n",
    "            dh = loss * sigmoid_gradient(prediction)\n",
    "            print(f\"{it} train prediction: \", prediction)\n",
    "            print(f\"{it} train target: \", targets)\n",
    "\n",
    "            gradients = model.compute_gradient(loss=loss, prediction=prediction, forward_cache=forward_cache)\n",
    "\n",
    "            model.apply_gradients(gradients)\n",
    "\n",
    "            epoch_training_loss += loss\n",
    "            it += 1\n",
    "\n",
    "        sentence_size = len(X_valid) \n",
    "        it = 0\n",
    "        for sentence, targets in zip(X_valid, Y_valid):            \n",
    "            forward_cache, prediction = model.forward(sentence=sentence)\n",
    "        \n",
    "            loss = bce(y, prediction).numpy() \n",
    "            # print(\"loss: \", loss)\n",
    "            print(f\"{it} loss: \", loss)\n",
    "            dh = loss * sigmoid_gradient(prediction)\n",
    "            print(f\"{it} validation prediction: \", prediction)\n",
    "            print(f\"{it} validation target: \", targets)\n",
    "#             print(f\"{it} valid prediction: \", dh)\n",
    "            epoch_training_loss += loss\n",
    "            it += 1\n",
    "#             print(f\"Epoch {i}, {it} out of {sentence_size} loss: \", loss)\n",
    "\n",
    "\n",
    "\n",
    "        # Save loss for plot\n",
    "        training_loss.append(epoch_training_loss / len(X_train))\n",
    "        validation_loss.append(epoch_validation_loss / len(X_valid))\n",
    "\n",
    "        # Print loss every 2 epochs\n",
    "        import statistics\n",
    "        print(f'Epoch {i+1}, training loss: {statistics.mean(training_loss)}, validation loss: {statistics.mean(validation_loss)}')\n",
    "\n",
    "        print(output_softmax)\n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fa0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================FINAL=====================\n",
    "Preparing the dataset\n",
    "\"\"\"\n",
    "\n",
    "def convert(x):\n",
    "    \"\"\"\n",
    "    Coverting JSON to pandas dataframe\n",
    "\n",
    "    \"\"\"    \n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "\n",
    "\n",
    "def filter_data(data):\n",
    "    \"\"\"\n",
    "    Converting into pandas dataframe and filtering only text and ratings given by the users\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame([convert(line) for line in data])\n",
    "    df.drop(columns=df.columns.difference(['text','stars']),inplace=True)\n",
    "    df.loc[:, (\"sentiment\")] = 0\n",
    "    \n",
    "\n",
    "#I have considered a rating above 3 as positive and less than or equal to 3 as negative.\n",
    "    df.loc[:,'sentiment']=['pos' if (x>3) else 'neg' for x in df.loc[:, 'stars']]\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    for idx,row in df.iterrows():\n",
    "        df.loc[:,'text']= [x for x in df.loc[:,'text']]\n",
    "    return df\n",
    "\n",
    "def min_max_normalize(tokens):\n",
    "    min_val = min(tokens)\n",
    "    max_val = max(tokens)\n",
    "    normalized_tokens = [(token - min_val) / (max_val - min_val) for token in tokens]\n",
    "    return normalized_tokens\n",
    "\n",
    "def read_data():\n",
    "    json_filename = 'review_mockup_500.json'\n",
    "    with open(json_filename,'rb') as f:\n",
    "        data = f.readlines()\n",
    "    data = filter_data(data)\n",
    "    tokenizer = Tokenizer(num_words = 2500, split=' ')\n",
    "    tokenizer.fit_on_texts(data.loc[:,'text'].values)\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(data.loc[:,'text'].values)\n",
    "    X = [min_max_normalize(i) for i in X]\n",
    "    test = pad_sequences(X)\n",
    "\n",
    "    Y = pd.get_dummies(data['sentiment'], dtype=int).values[:, 0]   \n",
    "    return X, Y\n",
    "# print(X)\n",
    "X, Y = read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbcd68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  0.819140435092852\n",
      "0 train prediction:  [[0.23787375]]\n",
      "0 train target:  0\n",
      "1 loss:  0.5912995931399148\n",
      "1 train prediction:  [[0.21525675]]\n",
      "1 train target:  1\n",
      "2 loss:  0.8241664749006343\n",
      "2 train prediction:  [[0.2468441]]\n",
      "2 train target:  0\n",
      "3 loss:  0.7725642077709066\n",
      "3 train prediction:  [[0.15298839]]\n",
      "3 train target:  0\n",
      "4 loss:  0.5431538521117993\n",
      "4 train prediction:  [[0.32652348]]\n",
      "4 train target:  1\n",
      "5 loss:  0.8899203338122026\n",
      "5 train prediction:  [[0.36112001]]\n",
      "5 train target:  0\n",
      "6 loss:  0.9535191673463854\n",
      "6 train prediction:  [[0.46676416]]\n",
      "6 train target:  0\n",
      "7 loss:  0.5369017702436153\n",
      "7 train prediction:  [[0.34150698]]\n",
      "7 train target:  1\n",
      "8 loss:  0.5861373001650959\n",
      "8 train prediction:  [[0.22685845]]\n",
      "8 train target:  1\n",
      "9 loss:  0.7844334284550158\n",
      "9 train prediction:  [[0.17493193]]\n",
      "9 train target:  0\n",
      "10 loss:  0.7855754662332533\n",
      "10 train prediction:  [[0.17703172]]\n",
      "10 train target:  0\n",
      "11 loss:  0.8374005836367895\n",
      "11 train prediction:  [[0.27029702]]\n",
      "11 train target:  0\n",
      "12 loss:  0.800407878253518\n",
      "12 train prediction:  [[0.20412289]]\n",
      "12 train target:  0\n",
      "13 loss:  0.8144876808895254\n",
      "13 train prediction:  [[0.2295379]]\n",
      "13 train target:  0\n",
      "14 loss:  0.8495597346986867\n",
      "14 train prediction:  [[0.291637]]\n",
      "14 train target:  0\n",
      "15 loss:  0.8744574142076114\n",
      "15 train prediction:  [[0.33473791]]\n",
      "15 train target:  0\n",
      "16 loss:  0.8567559975571681\n",
      "16 train prediction:  [[0.30417557]]\n",
      "16 train target:  0\n",
      "17 loss:  0.8570957655049447\n",
      "17 train prediction:  [[0.30476592]]\n",
      "17 train target:  0\n",
      "18 loss:  0.7995782433060146\n",
      "18 train prediction:  [[0.20261629]]\n",
      "18 train target:  0\n",
      "19 loss:  0.8555803407422065\n",
      "19 train prediction:  [[0.3021317]]\n",
      "19 train target:  0\n",
      "20 loss:  0.601348027468911\n",
      "20 train prediction:  [[0.19288509]]\n",
      "20 train target:  1\n",
      "21 loss:  0.8267378131113159\n",
      "21 train prediction:  [[0.25141974]]\n",
      "21 train target:  0\n",
      "22 loss:  0.7937385362055671\n",
      "22 train prediction:  [[0.19198251]]\n",
      "22 train target:  0\n",
      "23 loss:  0.7821923909284518\n",
      "23 train prediction:  [[0.17080563]]\n",
      "23 train target:  0\n",
      "24 loss:  0.7912091253805208\n",
      "24 train prediction:  [[0.18736069]]\n",
      "24 train target:  0\n",
      "25 loss:  0.8264763885078259\n",
      "25 train prediction:  [[0.25095496]]\n",
      "25 train target:  0\n",
      "26 loss:  0.6091185086540937\n",
      "26 train prediction:  [[0.17577131]]\n",
      "26 train target:  1\n",
      "27 loss:  0.8278637002304814\n",
      "27 train prediction:  [[0.25342035]]\n",
      "27 train target:  0\n",
      "28 loss:  0.5731383999828272\n",
      "28 train prediction:  [[0.25640914]]\n",
      "28 train target:  1\n",
      "29 loss:  0.8286634734456535\n",
      "29 train prediction:  [[0.25484042]]\n",
      "29 train target:  0\n",
      "30 loss:  0.8291506477689433\n",
      "30 train prediction:  [[0.25570501]]\n",
      "30 train target:  0\n",
      "31 loss:  0.7859469144490129\n",
      "31 train prediction:  [[0.17771425]]\n",
      "31 train target:  0\n",
      "32 loss:  0.7961839644470403\n",
      "32 train prediction:  [[0.1964417]]\n",
      "32 train target:  0\n",
      "33 loss:  0.7919728458231825\n",
      "33 train prediction:  [[0.1887572]]\n",
      "33 train target:  0\n",
      "34 loss:  0.7718209797133431\n",
      "34 train prediction:  [[0.15160693]]\n",
      "34 train target:  0\n",
      "35 loss:  0.8236929357878442\n",
      "35 train prediction:  [[0.24600045]]\n",
      "35 train target:  0\n",
      "36 loss:  0.8288483658408674\n",
      "36 train prediction:  [[0.25516859]]\n",
      "36 train target:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Number of units in the hidden state/memory cell\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(units\u001b[38;5;241m=\u001b[39munits, input_dim\u001b[38;5;241m=\u001b[39minput_dim)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmy_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 33\u001b[0m, in \u001b[0;36mmy_train\u001b[0;34m(model, X, Y, hidden_size, input_size, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train prediction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train target: \u001b[39m\u001b[38;5;124m\"\u001b[39m, targets)\n\u001b[0;32m---> 33\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mapply_gradients(gradients)\n\u001b[1;32m     37\u001b[0m epoch_training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[54], line 184\u001b[0m, in \u001b[0;36mMY_LSTM.compute_gradient\u001b[0;34m(self, loss, prediction, forward_cache)\u001b[0m\n\u001b[1;32m    180\u001b[0m d_candidate_gate \u001b[38;5;241m=\u001b[39m dC \u001b[38;5;241m*\u001b[39m input_gate[t] \u001b[38;5;241m*\u001b[39m tanh_gradient(candidate_gate[t]) \n\u001b[1;32m    182\u001b[0m d_input_gate \u001b[38;5;241m=\u001b[39m dC \u001b[38;5;241m*\u001b[39m candidate_gate[t] \u001b[38;5;241m*\u001b[39m sigmoid_gradient(input_gate[t]) \n\u001b[0;32m--> 184\u001b[0m d_forget_gate \u001b[38;5;241m=\u001b[39m dC \u001b[38;5;241m*\u001b[39m ltm[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43msigmoid_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforget_gate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    186\u001b[0m d_gates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d_input_gate, d_forget_gate, d_candidate_gate, d_output_gate])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m#Looks reasonable\u001b[39;00m\n\u001b[1;32m    187\u001b[0m d_kernel \u001b[38;5;241m=\u001b[39m (d_gates \u001b[38;5;241m*\u001b[39m forward_cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m][t])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m,)\n",
      "Cell \u001b[0;32mIn[47], line 10\u001b[0m, in \u001b[0;36msigmoid_gradient\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid_gradient\u001b[39m(x):\n\u001b[0;32m---> 10\u001b[0m     sigmoid_x \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sigmoid_x \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m sigmoid_x)\n",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(X):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/keras/activations.py:403\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.activations.sigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[1;32m    379\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sigmoid activation function, `sigmoid(x) = 1 / (1 + exp(-x))`.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m  Applies the sigmoid activation function. For small values (<-5),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m      Tensor with the sigmoid activation: `1 / (1 + exp(-x))`.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m   \u001b[38;5;66;03m# Cache the logits to use for crossentropy loss.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m   output\u001b[38;5;241m.\u001b[39m_keras_logits \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:4192\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   4190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, [x]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m   4191\u001b[0m   x \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4192\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:9457\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   9455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   9456\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 9457\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9458\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSigmoid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   9460\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def build_model(units, input_dim):\n",
    "    lstm = MY_LSTM(units=units)\n",
    "    lstm.build(input_dim)\n",
    "    return lstm\n",
    "\n",
    "np.random.seed(228)\n",
    "batch_size = 1  # Number of training examples\n",
    "input_dim = 1  # Number of features in the input\n",
    "units = 4  # Number of units in the hidden state/memory cell\n",
    "model = build_model(units=units, input_dim=input_dim)\n",
    "\n",
    "my_train(model=model, X=X, Y=Y, hidden_size=units, input_size=input_dim, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cef416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
