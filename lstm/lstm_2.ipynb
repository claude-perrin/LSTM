{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2894677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#  import matplotlib.pyplot as plt\n",
    "from activation_functions import tanh_activation, sigmoid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# HyperParameters:\n",
    "embed_dim = 14\n",
    "lstm_out = 50\n",
    "batch_size = 32\n",
    "num_words = 2500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600d97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing dataset\n",
    "\n",
    "def convert(x):\n",
    "    \"\"\"\n",
    "    Coverting JSON to pandas dataframe\n",
    "\n",
    "    \"\"\"    \n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(v)\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "\n",
    "\n",
    "def filter_data(data):\n",
    "    \"\"\"\n",
    "    Converting into pandas dataframe and filtering only text and ratings given by the users\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame([convert(line) for line in data])\n",
    "    df.drop(columns=df.columns.difference(['text','stars']),inplace=True)\n",
    "    df.loc[:, (\"sentiment\")] = 0\n",
    "    \n",
    "\n",
    "#     #I have considered a rating above 3 as positive and less than or equal to 3 as negative.\n",
    "    df.loc[:,'sentiment']=['pos' if (x>3) else 'neg' for x in df.loc[:, 'stars']]\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply(lambda x: x.lower())\n",
    "    df.loc[:,'text'] = df.loc[:,'text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    for idx,row in df.iterrows():\n",
    "        df.loc[:,'text']= [x for x in df.loc[:,'text']]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb484233",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = 'review_mockup.json'\n",
    "with open(json_filename,'rb') as f:\n",
    "    data = f.readlines()\n",
    "data = filter_data(data)\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "tokenizer.fit_on_texts(data.loc[:,'text'].values)\n",
    "#print(tokenizer.word_index)  # To see the dicstionary\n",
    "X = tokenizer.texts_to_sequences(data.loc[:,'text'].values)\n",
    "X = pad_sequences(X)\n",
    "# print((X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50414f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_LSTM:\n",
    "    def __init__(self, hidden_size, input_size, optimizer, loss_func):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = 2\n",
    "        self._optimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self.USE_OPTIMIZER = True\n",
    "\n",
    "\n",
    "        # input_size = 2\n",
    "        # hidden_size = 1\n",
    "        # z_size = 24\n",
    "        z_size = self.hidden_size + self.input_size\n",
    "        \"\"\"\n",
    "        Init\n",
    "        \n",
    "        \"\"\"\n",
    "        self.parameters = {\n",
    "            \"weights\": {\n",
    "                \"W_Forget\": self.__init_orthogonal(np.zeros((self.hidden_size, z_size))),\n",
    "                \"W_Input\": self.__init_orthogonal(np.zeros((self.hidden_size, z_size))),\n",
    "                \"W_Candidate\": self.__init_orthogonal(np.zeros((self.hidden_size, z_size))),\n",
    "                \"W_Output\": self.__init_orthogonal(np.zeros((self.hidden_size, z_size))),\n",
    "                \"W_OutputSoftmax\": self.__init_orthogonal(np.zeros((self.hidden_size, self.output_size))),\n",
    "\n",
    "            },\n",
    "            \"bias\": {\n",
    "                \"b_Forget\": np.ones((self.hidden_size, 1)),\n",
    "                \"b_Input\": np.ones((self.hidden_size, 1)),\n",
    "                \"b_Candidate\": np.ones((self.hidden_size, 1)),\n",
    "                \"b_Output\": np.ones((self.hidden_size, 1)),\n",
    "                \"b_OutputSoftmax\": np.ones((self.input_size, 1)),\n",
    "\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, optimizer):\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "\n",
    "    @property\n",
    "    def loss_func(self):\n",
    "        return self._loss_func\n",
    "\n",
    "    @loss_func.setter\n",
    "    def loss_func(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def __init_orthogonal(self, param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "        This is a common initiailization for recurrent neural networks.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "            https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "\n",
    "    @property\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns weights and biases as 2d array\n",
    "\n",
    "        \"\"\"\n",
    "        return self.parameters\n",
    "\n",
    "    def forward(self, input_data, prev_stm, prev_ltm):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_data -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "            prev_stm -- h at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "            prev_ltm -- c at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "        Returns:\n",
    "            outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "            \n",
    "            Weight shape:\n",
    "            All of the weights    shape (self.hidden_size, z_size)\n",
    "            Except W_OutputSoftmax      (self.hidden_size, self.output_size)\n",
    "            \n",
    "            Data shapes:\n",
    "            input_data            shape (1,1)\n",
    "            prev_stm and prev_ltm shape (1,hidden_state)\n",
    "            concat                shape is (1, z_size)\n",
    "            concat.T              shape is (z_size, 1)\n",
    "             - forget_gate        shape is (hidden_size, 1)\n",
    "             - input_gate         shape is (hidden_size, 1)\n",
    "             - candidate          shape is (hidden_size, 1)\n",
    "             - next_ltm           shape is (1, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Save a list of computations for each of the components in the LSTM\n",
    "        \n",
    "        concat = np.concatenate((prev_stm, input_data), axis=1)\n",
    "#         print(f\"Concat value is: \\n\", concat, \"\\nConcat.T value is: \\n\", concat.T)\n",
    "        \n",
    "        # Compute the forget gate\n",
    "#         print(\"=======Computation of the forget gate:======\")\n",
    "#         print(f\"W_forget:\\n {self.parameters['weights']['W_Forget']}\\nconcat.T:\\n {concat.T}\\nb_Forget: {self.parameters['bias']['b_Forget']}\")\n",
    "        forget_gate = sigmoid(np.dot(self.parameters[\"weights\"][\"W_Forget\"], concat.T) + self.parameters[\"bias\"][\"b_Forget\"])\n",
    "#         print(f\"Forget_gate value is: \\n\", forget_gate)\n",
    "#         print(\"===========================================\")\n",
    "\n",
    "\n",
    "        # Compute the input gate\n",
    "        input_gate = sigmoid(np.dot(self.parameters[\"weights\"][\"W_Input\"], concat.T) + self.parameters[\"bias\"][\"b_Input\"])\n",
    "#         print(f\"Input_gate value is: \\n\", input_gate)\n",
    "\n",
    "        # Compute the candidate cell value\n",
    "        candidate = np.tanh(np.dot(self.parameters[\"weights\"][\"W_Candidate\"], concat.T) + self.parameters[\"bias\"][\"b_Candidate\"])\n",
    "#         print(f\"Candidate_gate value is: \\n\", candidate)\n",
    "\n",
    "        # Compute the memory cell\n",
    "#         print(f\"==============next_ltm===========\\n forget_gate * prev_ltm:\\n {forget_gate * prev_ltm}\")\n",
    "#         print(f\"input_gate * candidate: \\n {input_gate * candidate}\")\n",
    "        next_ltm = forget_gate * prev_ltm.T + input_gate * candidate\n",
    "#         print(f\"next_ltm value is: \\n\", next_ltm)\n",
    "#         print(\"==================================\")\n",
    "\n",
    "\n",
    "        # Compute the output gate\n",
    "        output_gate = sigmoid(np.dot(self.parameters[\"weights\"][\"W_Output\"], concat.T) + self.parameters[\"bias\"][\"b_Output\"])\n",
    "#         print(f\"Output_gate value is: \\n\", output_gate)\n",
    "\n",
    "        # Compute the next hidden state\n",
    "        next_stm = output_gate * np.tanh(next_ltm)\n",
    "#         print(f\"next_stm value is: \\n\", next_stm)\n",
    "\n",
    "        forward_pass = {\n",
    "            \"Forget\": forget_gate,\n",
    "            \"Input\": input_gate,\n",
    "            \"Candidate\": candidate,\n",
    "            \"Output\": output_gate,\n",
    "            \"next_ltm\": next_ltm.T,\n",
    "            \"next_stm\": next_stm.T,\n",
    "            \"Concat_Input\": concat\n",
    "        }\n",
    "        \n",
    "        return forward_pass\n",
    "\n",
    "    def __cross_entropy(self, predictions, targets, epsilon=1e-12):\n",
    "        \"\"\"\n",
    "        Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "        and predictions.\n",
    "        Input: predictions (N, k) ndarray\n",
    "               targets (N, k) ndarray\n",
    "        Returns: scalar\n",
    "        \"\"\"\n",
    "        predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "        N = predictions.shape[0]\n",
    "        ce = -np.sum(targets * np.log(predictions + 1e-9)) / N\n",
    "        return ce\n",
    "\n",
    "\n",
    "    def calculate_loss(self, prediction, targets):\n",
    "        print(f\"============IN LOSS=============\")\n",
    "        print(f\"prediction: {prediction}\")\n",
    "        print(f\"targets: {targets}\")\n",
    "\n",
    "        return self.loss_func(prediction, targets)\n",
    "\n",
    "    def backward(self, forward_pass, prediction, targets):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        forward_pass -- dictionary:\n",
    "                        \"Forget_gate\": forget_gate,\n",
    "                        \"Input_gate\": input_gate,\n",
    "                        \"Candidate_gate\": candidate,\n",
    "                        \"Output_gate\": output_gate,\n",
    "                        \"next_ltm\": next_ltm.T,\n",
    "                        \"next_stm\": next_stm.T\n",
    "                        \n",
    "        targets -- your targets as a list of size m.\n",
    "        Returns:\n",
    "        loss -- crossentropy loss for all elements in output\n",
    "        grads -- lists of gradients of every element in p\n",
    "        \n",
    "        Weight shape:\n",
    "            All of the weights    shape (self.hidden_size, z_size)\n",
    "            Except W_OutputSoftmax      (self.hidden_size, self.output_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        gradients = {\n",
    "            \"weights\": {\n",
    "                \"W_Forget\": np.zeros_like(self.parameters[\"weights\"][\"W_Forget\"]),\n",
    "                \"W_Input\": np.zeros_like(self.parameters[\"weights\"][\"W_Input\"]),\n",
    "                \"W_Candidate\": np.zeros_like(self.parameters[\"weights\"][\"W_Candidate\"]),\n",
    "                \"W_Output\": np.zeros_like(self.parameters[\"weights\"][\"W_Output\"]),\n",
    "                \"W_OutputSoftmax\": np.zeros_like(self.parameters[\"weights\"][\"W_OutputSoftmax\"]),\n",
    "            }\n",
    "#             \"bias\": {\n",
    "#                 \"b_Forget\": np.zeros_like(self.parameters[\"bias\"][\"b_Forget\"]),\n",
    "#                 \"b_Input\": np.zeros_like(self.parameters[\"bias\"][\"b_Input\"]),\n",
    "#                 \"b_Candidate\": np.zeros_like(self.parameters[\"bias\"][\"b_Candidate\"]),\n",
    "#                 \"b_Output\": np.zeros_like(self.parameters[\"bias\"][\"b_Output\"]),\n",
    "#                 \"b_OutputSoftmax\": np.zeros_like(self.parameters[\"bias\"][\"b_OutputSoftmax\"]),\n",
    "#             }\n",
    "        }\n",
    "\n",
    "        # Set the next cell and hidden state equal to zero\n",
    "        print(f\"==============PERFORMING BACKWARD===========\\n\")\n",
    "        print(f\"INPUTS:\\nforward_pass: \\n{forward_pass}\\ntargets: \\n{targets}\\nprediction: \\n{prediction}\")\n",
    "        print(f\"\\n==============END OF INPUTS=================\\n\")\n",
    "        \n",
    "        print(\"\\n===============PROCESSING------------------\\n\")\n",
    "#         next_stm = np.zeros_like(forward_pass[\"next_stm\"])  # h\n",
    "#         next_ltm = np.zeros_like(forward_pass[\"next_ltm\"])  # C\n",
    "        next_ltm = forward_pass[\"next_ltm\"]\n",
    "        next_stm = forward_pass['next_stm']\n",
    "\n",
    "        loss = 0\n",
    "        # Compute the cross entropy\n",
    "        \n",
    "        for t in reversed(range(self.hidden_size)):\n",
    "\n",
    "            loss += self.loss_func(prediction[0].tolist(), targets)\n",
    "            print(f\"[{t}] LOSS:  {loss}\")\n",
    "            # Get the previous hidden cell state\n",
    "\n",
    "\n",
    "\n",
    "            # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "            dv = np.copy(prediction)\n",
    "            dv[np.argmax(targets)] -= 1\n",
    "            print(f\"[{t}] dv (OUTPUT_GATE):  \\n{dv}\\n\")\n",
    "            # Update the gradient of the relation of the hidden-state to the output gate\n",
    "            print(f\"[{t}] dv (OUTPUT_GATE):  \\n{dv}\\n\")\n",
    "            print(f\"[{t}] gradients[W_OutputSoftmax]:  \\n{gradients['weights']['W_OutputSoftmax']}\\n\")\n",
    "            gradients[\"weights\"][\"W_OutputSoftmax\"] += np.dot(next_stm.T, dv)\n",
    "            #gradients[\"bias\"][\"b_OutputSoftmax\"] += dv.T \n",
    "\n",
    "\n",
    "            # Compute the derivative of the hidden state and output gate\n",
    "            dh = np.dot(dv, self.parameters[\"weights\"][\"W_OutputSoftmax\"].T)\n",
    "            print(f\"[{t}] dh:  \\n{gradients['weights']['W_OutputSoftmax']}\\n\")\n",
    "            dh += next_stm\n",
    "\n",
    "            do = dh * tanh_activation(next_ltm)\n",
    "            print(f\"[{t}] do: BEFORE SIGMOID \\n{do}\\n\")\n",
    "            do = sigmoid(forward_pass[\"Output\"], derivative=True) * do\n",
    "            print(f\"[{t}] do: AFTER SIGMOID \\n{do}\\n\")\n",
    "\n",
    "            # Update the gradients with respect to the output gate\n",
    "            print(f\"[{t}] gradients[W_Output] BEFORE UPDATE:  \\n{gradients['weights']['W_Output']}\\n\")\n",
    "            # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "            gradients[\"weights\"][\"W_Output\"] += np.dot(do, forward_pass[\"Concat_Input\"].T)\n",
    "            #gradients[\"bias\"][\"b_Output\"] += do\n",
    "            print(f\"[{t}] gradients[W_Output] AFTER UPDATE:  \\n{gradients['weights']['W_Output']}\\n\")\n",
    "\n",
    "\n",
    "            # Compute the derivative of the cell state and candidate g\n",
    "            dC = np.copy(ltm_next)\n",
    "            dC += dh * forward_pass[\"Output\"] * tanh_activation(next_ltm, derivative=True)\n",
    "            print(f\"[{t}] dC:  \\n{dC}\\n\")\n",
    "\n",
    "            dg = dC * forward_pass[\"Input\"]\n",
    "            dg = tanh_activation(forward_pass[\"Candidate\"], derivative=True) * dg\n",
    "            print(f\"[{t}] dg:  \\n{dg}\\n\")\n",
    "\n",
    "\n",
    "            # Update the gradients with respect to the candidate\n",
    "            print(f\"[{t}] gradients[W_Candidate] BEFORE UPDATE:  \\n{gradients['weights']['W_Candidate']}\\n\")\n",
    "            # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "            gradients[\"weights\"][\"W_Candidate\"] += np.dot(dg, forward_pass[\"Concat_Input\"].T)\n",
    "            #gradients[\"bias\"][\"b_Output\"] += dg\n",
    "            print(f\"[{t}] gradients[W_Candidate] AFTER UPDATE:  \\n{gradients['weights']['W_Candidate']}\\n\")\n",
    "\n",
    "\n",
    "            # Compute the derivative of the input gate and update its gradients\n",
    "            di = dC * forward_pass[\"Candidate\"]\n",
    "            di = sigmoid(forward_pass[\"Input\"], True) * di\n",
    "            print(f\"[{t}] di:  \\n{di}\\n\")\n",
    "\n",
    "            print(f\"[{t}] gradients[W_Input] BEFORE UPDATE:  \\n{gradients['weights']['W_Input']}\\n\")\n",
    "            # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "            gradients[\"weights\"][\"W_Input\"] += np.dot(di, forward_pass[\"Concat_Input\"].T)\n",
    "            #gradients[\"bias\"][\"b_Input\"] += di\n",
    "            print(f\"[{t}] gradients[W_Input] AFTER UPDATE:  \\n{gradients['weights']['W_Input']}\\n\")\n",
    "\n",
    "\n",
    "            # Compute the derivative of the forget gate and update its gradients\n",
    "            df = dC * ltm_prev\n",
    "            df = sigmoid(forward_pass[\"Forget\"][t]) * df\n",
    "            print(f\"[{t}] df:  \\n{df}\\n\")\n",
    "\n",
    "\n",
    "            print(f\"[{t}] gradients[W_Forget] BEFORE UPDATE:  \\n{gradients['weights']['W_Forget']}\\n\")\n",
    "            # =========================== ?????????DO WE NEED Concat_Input?????????????====================\n",
    "            gradients[\"weights\"][\"W_Forget\"] += np.dot(df, forward_pass[\"Concat_Input\"].T)\n",
    "            #gradients[\"bias\"][\"b_Forget\"] += df\n",
    "            print(f\"[{t}] gradients[W_Forget] AFTER UPDATE:  \\n{gradients['weights']['W_Forget']}\\n\")\n",
    "\n",
    "\n",
    "            # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "            dz = (np.dot(self.parameters[\"weights\"][\"W_Forget\"].T, df) + np.dot(self.parameters[\"weights\"][\"W_Input\"].T, di) + np.dot(\n",
    "                self.parameters[\"weights\"][\"W_Candidate\"].T, dg) + np.dot(self.parameters[\"weights\"][\"W_Output\"].T, do))\n",
    "            print(f\"[{t}] dz:  \\n{dz}\\n\")\n",
    "\n",
    "            dh_prev = dz[:self.hidden_size, :]\n",
    "            print(f\"[{t}] dh_prev:  \\n{dh_prev}\\n\")\n",
    "\n",
    "            dC_prev = forward_pass[\"Forget\"] * dC\n",
    "            print(f\"[{t}] dC_prev:  \\n{dC_prev}\\n\")\n",
    "\n",
    "\n",
    "            # Clip gradients\n",
    "            print(f\"=========\\nGRADS BEFORE CLIP:  \\n{grads}\\n\")\n",
    "\n",
    "            grads = self.__clip_gradient_norm(gradients)\n",
    "\n",
    "            print(f\"=========\\nGRADS AFTER CLIP:  \\n{grads}\\n\")\n",
    "\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def __clip_gradient_norm(self, grads, max_norm=0.25):\n",
    "        \"\"\"\n",
    "        Clips gradients to have a maximum norm of `max_norm`.\n",
    "        This is to prevent the exploding gradients problem.\n",
    "        \"\"\"\n",
    "        # Set the maximum of the norm to be of type float\n",
    "        max_norm = float(max_norm)\n",
    "        total_norm = 0\n",
    "        # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "        for gate, grad in grads[\"weights\"].items():\n",
    "            grad_norm = np.sum(np.power(grad, 2))\n",
    "            total_norm += grad_norm\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        # Calculate clipping coeficient\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "        if clip_coef < 1:\n",
    "            for gate, grad in grads[\"weights\"].items():\n",
    "                grad *= clip_coef\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, t, lr=0.01):\n",
    "        # Take a step\n",
    "        parameters = self.get_parameters[\"weights\"]\n",
    "        if (self.USE_OPTIMIZER):\n",
    "            updated_parameters = self.optimizer(parameters=parameters, gradients=grads, learning_rate=lr, t=t)\n",
    "        else:\n",
    "            for (_, parameter), (_, grad) in zip(parameters.items(), grads.items()):\n",
    "                parameter -= lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae19c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========Printing for MY_LSTM===============\n",
      "\n",
      "INPUTS\n",
      "\n",
      "(x) Input:\n",
      " [[9]]\n",
      "(prev_stm) {h} Previous hidden state:\n",
      " [[0. 0. 0. 0. 0. 0.]]\n",
      "(prev_ltm) {c} Previous memory cell:\n",
      " [[0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "=========PERFORM FORWARD PASS==============\n",
      "\n",
      "\n",
      "=========APPLY SOFTMAX (LAST LAYER OF LSTM)\n",
      "\n",
      "===========OUTPUT============================\n",
      "(next_ltm) {h} Next hidden state:\n",
      " [[-0.17110701  0.30377355  0.07750367 -0.20858678 -0.24969392  0.01808954]]\n",
      "(next_stm) {c} Next memory cell:\n",
      " [[-0.6403307   0.9078139   0.08001236 -0.21188491 -0.25719292  0.12117781]]\n",
      "(Output_gate) RESHAPED:\n",
      " [0.30277745 0.42186005 0.97071239 0.99912253 0.99215558 0.1500109 ]\n",
      "softmax(Output)\n",
      " [[0.51996451 0.48003549]]\n",
      "\n",
      "========END OF PRINTING FOR MY_LSTM========\n",
      "\n",
      "\n",
      "========START OF TENSORFLOW LSTM===========\n",
      "\n",
      "1\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Prediction is:  [[0.5001586  0.49984145]]\n",
      "\n",
      "========END OF TENSORFLOW LSTM=============\n",
      "\n",
      "\n",
      "\n",
      "======START OF BACKWARDPROPAGATION=======\n",
      "\n",
      "==============PERFORMING BACKWARD===========\n",
      "\n",
      "INPUTS:\n",
      "forward_pass: \n",
      "{'Forget': array([[0.32321709],\n",
      "       [0.98716645],\n",
      "       [0.11279658],\n",
      "       [0.00507193],\n",
      "       [0.67489138],\n",
      "       [0.04492773]]), 'Input': array([[0.8320444 ],\n",
      "       [0.9916356 ],\n",
      "       [0.08001527],\n",
      "       [0.55398839],\n",
      "       [0.25725837],\n",
      "       [0.12332961]]), 'Candidate': array([[-0.76958718],\n",
      "       [ 0.91547126],\n",
      "       [ 0.99996369],\n",
      "       [-0.38247175],\n",
      "       [-0.99974559],\n",
      "       [ 0.98255241]]), 'Output': array([[0.30277745],\n",
      "       [0.42186005],\n",
      "       [0.97071239],\n",
      "       [0.99912253],\n",
      "       [0.99215558],\n",
      "       [0.1500109 ]]), 'next_ltm': array([[-0.6403307 ,  0.9078139 ,  0.08001236, -0.21188491, -0.25719292,\n",
      "         0.12117781]]), 'next_stm': array([[-0.17110701,  0.30377355,  0.07750367, -0.20858678, -0.24969392,\n",
      "         0.01808954]]), 'Concat_Input': array([[0., 0., 0., 0., 0., 0., 9.]])}\n",
      "targets: \n",
      "[[1.0], [0.0]]\n",
      "prediction: \n",
      "[[0.91693763 0.8370371 ]]\n",
      "\n",
      "==============END OF INPUTS=================\n",
      "\n",
      "\n",
      "===============PROCESSING------------------\n",
      "\n",
      "LOSS:  0.35376523078210254\n",
      "dv (OUTPUT_GATE):  \n",
      "[[-0.08306237 -0.1629629 ]]\n",
      "\n",
      "dv (OUTPUT_GATE):  \n",
      "[[-0.08306237 -0.1629629 ]]\n",
      "\n",
      "gradients[W_OutputSoftmax]:  \n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "dh:  \n",
      "[[ 0.01421255  0.0278841 ]\n",
      " [-0.02523215 -0.04950382]\n",
      " [-0.00643764 -0.01263022]\n",
      " [ 0.01732571  0.03399191]\n",
      " [ 0.02074017  0.04069085]\n",
      " [-0.00150256 -0.00294792]]\n",
      "\n",
      "do: BEFORE SIGMOID \n",
      "[[ 0.16275967  0.22581601 -0.00216754  0.04324224  0.0807702   0.00948094]]\n",
      "\n",
      "do: AFTER SIGMOID \n",
      "[[ 0.03977143  0.05517967 -0.00052965  0.01056653  0.01973675  0.00231673]\n",
      " [ 0.03893194  0.05401495 -0.00051847  0.0103435   0.01932014  0.00226783]\n",
      " [ 0.03243105  0.04499548 -0.0004319   0.00861633  0.01609405  0.00188915]\n",
      " [ 0.03201347  0.04441612 -0.00042634  0.00850539  0.01588682  0.00186482]\n",
      " [ 0.03211632  0.04455882 -0.00042771  0.00853271  0.01593786  0.00187081]\n",
      " [ 0.04046186  0.05613759 -0.00053885  0.01074997  0.02007937  0.00235695]]\n",
      "\n",
      "gradients[W_Output] BEFORE UPDATE:  \n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (6,6) and (7,1) not aligned: 6 (dim 1) != 7 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========END OF TENSORFLOW LSTM=============\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m======START OF BACKWARDPROPAGATION=======\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_softmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 271\u001b[0m, in \u001b[0;36mMY_LSTM.backward\u001b[0;34m(self, forward_pass, prediction, targets)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradients[W_Output] BEFORE UPDATE:  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgradients[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_Output\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# =========================== ?????????DO WE NEED Concat_Input?????????????====================\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m gradients[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW_Output\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_pass\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConcat_Input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m#gradients[\"bias\"][\"b_Output\"] += do\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradients[W_Output] AFTER UPDATE:  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgradients[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_Output\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (6,6) and (7,1) not aligned: 6 (dim 1) != 7 (dim 0)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TESTING OF FORWARD PASS WITH DUMMY VALUES\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(1337)\n",
    "# Example data dimensions\n",
    "batch_size = 1  # Number of training examples\n",
    "input_size = 1  # Number of features in the input\n",
    "hidden_state = 6  # Number of units in the hidden state/memory cell\n",
    "model = my_build_model(hidden_size=hidden_state, input_size = input_size)\n",
    "n_y = 1  # Number of units in the output\n",
    "\n",
    "z_size = hidden_state + input_size\n",
    "# Generate example input data, previous hidden state, and previous memory cell\n",
    "x = np.array([[9]])\n",
    "prev_stm = np.zeros((batch_size, hidden_state))\n",
    "prev_ltm = np.zeros((batch_size, hidden_state))\n",
    "print(\"\\n=========Printing for MY_LSTM===============\\n\")\n",
    "print(\"INPUTS\\n\")\n",
    "print(\"(x) Input:\\n\", x)\n",
    "print(\"(prev_stm) {h} Previous hidden state:\\n\", prev_stm)\n",
    "print(\"(prev_ltm) {c} Previous memory cell:\\n\", prev_ltm)\n",
    "print(\"\\n=========PERFORM FORWARD PASS==============\\n\")\n",
    "\n",
    "# Perform forward pass\n",
    "print(\"\\n=========APPLY SOFTMAX (LAST LAYER OF LSTM)\\n\")\n",
    "forward_pass = model.forward(input_data=x, prev_stm=prev_stm, prev_ltm=prev_ltm)\n",
    "reshaped_output = forward_pass['Output'].reshape(hidden_state)\n",
    "\n",
    "# Print the output\n",
    "print(\"===========OUTPUT============================\")\n",
    "print(\"(next_ltm) {h} Next hidden state:\\n\", forward_pass[\"next_stm\"])\n",
    "print(\"(next_stm) {c} Next memory cell:\\n\", forward_pass[\"next_ltm\"])\n",
    "print(\"(Output_gate) RESHAPED:\\n\", forward_pass['Output'].reshape(hidden_state))\n",
    "output_softmax = np.dot(forward_pass[\"next_stm\"], model.parameters[\"weights\"][\"W_OutputSoftmax\"]) + model.parameters[\"bias\"][\"b_OutputSoftmax\"]\n",
    "print(f\"softmax(Output)\\n {tf.nn.softmax(output_softmax)}\")\n",
    "print(\"\\n========END OF PRINTING FOR MY_LSTM========\\n\")\n",
    "\n",
    "print(\"\\n========START OF TENSORFLOW LSTM===========\\n\")\n",
    "model_tf = build_model(x)\n",
    "print(\"Prediction is: \", model_tf.predict(x))\n",
    "print(\"\\n========END OF TENSORFLOW LSTM=============\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n======START OF BACKWARDPROPAGATION=======\\n\")\n",
    "model.backward(forward_pass=forward_pass, prediction=output_softmax, targets=[[1.0],[0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b69e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X):\n",
    "    model = Sequential()\n",
    "    print(X.shape[1])\n",
    "    model.add(Embedding(num_words, embed_dim,input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2164ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_build_model(hidden_size, input_size):\n",
    "    lstm = MY_LSTM(hidden_size = hidden_size, input_size = input_size, optimizer=0, loss_func=mean_squared_error)\n",
    "    return lstm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432ebbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_split(X, Y, test_size):\n",
    "    length = int(len(X) * test_size)\n",
    "    X_train = X[1:length]\n",
    "    X_valid = X[length:]\n",
    "    Y_train = Y[1:length]\n",
    "    Y_valid = Y[length:]\n",
    "    return X_train, X_valid, Y_train, Y_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb4b09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "Epoch 0, training loss: 0.001001001001001001, validation loss: 0.00025\n",
      "Input sentence 0:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0   92  850    3  197    7\n",
      " 1477  115    1  235    2  242    5    1 1477   30 1443   49    1   74\n",
      "  201  521    2  108    5    1 1041    2  169 1124    8    3  266 1042\n",
      "  211   38   10  471  132    5 1217   11    1  635 1374  444    9  496\n",
      "    5  947 1072    2  438   66  236    5  278   56  125   11   46  165\n",
      "  120   20   93   35    8   66  125    4   68  873   15  317  331  756\n",
      "    1  411    7 1443    3   38   10 2099    2    8   26   32    8  221\n",
      "   10   37    7  221 2445    5   75    8   89   11    5 1477    5  251\n",
      "    2    8  304    5    2 1478   49  471   89    3 1410   19  221  976\n",
      "   84   55  874   71   11 1124]\n",
      "\n",
      "Target sequence 0:\n",
      "0\n",
      "\n",
      "Predicted sequence:\n",
      "<generator object my_train.<locals>.<genexpr> at 0x1671b5580>\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "Epoch 10, training loss: 0.001001001001001001, validation loss: 0.00025\n",
      "Input sentence 10:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0   92  850    3  197    7\n",
      " 1477  115    1  235    2  242    5    1 1477   30 1443   49    1   74\n",
      "  201  521    2  108    5    1 1041    2  169 1124    8    3  266 1042\n",
      "  211   38   10  471  132    5 1217   11    1  635 1374  444    9  496\n",
      "    5  947 1072    2  438   66  236    5  278   56  125   11   46  165\n",
      "  120   20   93   35    8   66  125    4   68  873   15  317  331  756\n",
      "    1  411    7 1443    3   38   10 2099    2    8   26   32    8  221\n",
      "   10   37    7  221 2445    5   75    8   89   11    5 1477    5  251\n",
      "    2    8  304    5    2 1478   49  471   89    3 1410   19  221  976\n",
      "   84   55  874   71   11 1124]\n",
      "\n",
      "Target sequence 10:\n",
      "0\n",
      "\n",
      "Predicted sequence:\n",
      "<generator object my_train.<locals>.<genexpr> at 0x1671b5580>\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n",
      "targets inside training loop:           0\n",
      "1000  False\n",
      "1001   True\n",
      "1002  False\n",
      "1003  False\n",
      "1004   True\n",
      "...     ...\n",
      "4995  False\n",
      "4996  False\n",
      "4997   True\n",
      "4998  False\n",
      "4999   True\n",
      "\n",
      "[4000 rows x 1 columns]\n",
      "forward_pass['result'][-1] : [[1.]]\n",
      "targets inside loss func: [[0]]\n",
      "targets inside backward func: [[0]]\n",
      "Output.shape (300, 1)\n",
      "tanh.shape [1.]\n",
      "dc shape: (300, 1)\n",
      "dh shape: (300, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def my_train(lstm, X, Y, hidden_size, input_size):\n",
    "    \"\"\"\n",
    "    X are all sentences: [[9, 123, 5423, 121], [1,2,3,4]]\n",
    "    Y are all features:  [[1.0], [0.0]]\n",
    "    \n",
    "    \n",
    "    inputs is a sentence: [9, 123, 5423, 121]\n",
    "    targets is a feature : [1.0]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Hyper-parameters\n",
    "    num_epochs = 5\n",
    "\n",
    "    # Initialize hidden state as zeros\n",
    "    # hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "    X_train, X_valid, Y_train, Y_valid = train_split(X = X, Y = Y, test_size = 0.20)\n",
    "\n",
    "    # Track loss\n",
    "    training_loss, validation_loss = [], []\n",
    "\n",
    "    # For each epoch\n",
    "    for i in range(20):\n",
    "\n",
    "        # Track loss\n",
    "        epoch_training_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "\n",
    "        # For each sentence in validation set\n",
    "        \n",
    "        # inputs are sentences \n",
    "        for inputs, targets in zip(X_valid, Y_valid):\n",
    "            print(f\"targets inside training loop: {Y_valid}\" )\n",
    "            # Forward pass\n",
    "#             print(f\"input : {inputs} \")\n",
    "#             print(f\"input type: {type(inputs)} \")\n",
    "#             print(f\"targets: {targets} \")\n",
    "#             print(f\"targets shape: {type(targets)} \")\n",
    "          # Initialize hidden state and cell state as zeros\n",
    "            stm_prev = np.zeros((hidden_size, 1))\n",
    "            ltm_prev = np.zeros((hidden_size, 1))\n",
    "            \n",
    "            forward_pass = lstm.forward(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             print(f\"return of forward: {forward_pass}\")\n",
    "#             print(f\"forward_pass['result'] {forward_pass['result']}\")\n",
    "            print(f\"forward_pass['result'][-1] : {forward_pass['result'][-1]}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Backward pass\n",
    "            loss = lstm.calculate_loss(forward_pass[\"result\"][-1], [[targets]])\n",
    "\n",
    "            # Update loss\n",
    "            epoch_validation_loss += loss\n",
    "       \n",
    "        # For each sentence in training set\n",
    "        t = 1\n",
    "        for inputs, targets in zip(X_train, Y_train):\n",
    "\n",
    "            # One-hot encode input and target sequence\n",
    "            #  inputs_one_hot = dataset.one_hot_encode_sequence(inputs)\n",
    "            #  targets_one_hot = dataset.one_hot_encode_sequence(targets)\n",
    "\n",
    "       \n",
    "\n",
    "            # Forward pass\n",
    "            forward_pass = lstm.forward(inputs)\n",
    "\n",
    "            # Backward pass\n",
    "            loss, grads = lstm.backward(forward_pass, [[targets]])\n",
    "\n",
    "            # Update parameters\n",
    "\n",
    "            params = lstm.update_parameters(grads=grads[\"weights\"], t=t)\n",
    "            t += 1\n",
    "            # Update loss\n",
    "            #output_sentence = [dataset.idx_to_word[np.argmax(output)] for output in forward_pass[\"output_s\"]]\n",
    "\n",
    "            epoch_training_loss += loss\n",
    "\n",
    "        # Save loss for plot\n",
    "        training_loss.append(epoch_training_loss / len(X_train))\n",
    "        validation_loss.append(epoch_validation_loss / len(X_valid))\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "            print(f'Input sentence {i}:')\n",
    "            print(inputs)\n",
    "\n",
    "            print(f'\\nTarget sequence {i}:')\n",
    "            print(targets)\n",
    "\n",
    "            print('\\nPredicted sequence:')\n",
    "            print([np.argmax(output)] for output in forward_pass[\"result\"])\n",
    "    return training_loss, validation_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embed_dim = 12\n",
    "    lstm_out = 300\n",
    "    batch_size = 32\n",
    "    input_size = 200\n",
    "    #X = [[0,0,9,6,324,2,4,131,289,109,293,9],[2,84,67,60,74,97, 4,667,388,554,67,46,15]]\n",
    "    # Y = [[1], [0]]\n",
    "#     print(type(data))\n",
    "#     print(data['sentiment'])\n",
    "    Y = pd.get_dummies(data['sentiment']).values[:,0]   \n",
    "    G = pd.DataFrame(Y.reshape(-1, 1))\n",
    "#     print(X.shape)\n",
    "#     print(type(Y[0]))\n",
    "#     print(Y.shape)\n",
    "#     print(G.shape)\n",
    "    print(f\"===============\")\n",
    "#     print(f\"Y : {Y}\" )\n",
    "#     print(G.shape)\n",
    "    \n",
    "#     Y = data[1]\n",
    "    model = my_build_model(X, hidden_size=lstm_out, input_size = len(X[0]))\n",
    "    my_train(model, X, G, lstm_out, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc473311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': {'W_Forget': array([[ 0.04230445, -0.00828666,  0.02851241, ...,  0.04726605,\n",
      "         0.01827041, -0.02315239],\n",
      "       [ 0.05941409, -0.04375356, -0.06355351, ...,  0.00407919,\n",
      "        -0.14500059,  0.04052528],\n",
      "       [-0.00212553,  0.00600893, -0.01418321, ...,  0.10642387,\n",
      "         0.02766756, -0.05203458],\n",
      "       ...,\n",
      "       [-0.07516916, -0.12781673,  0.02946873, ..., -0.01015268,\n",
      "         0.05676914,  0.06283404],\n",
      "       [-0.0529668 ,  0.16381545,  0.09436224, ...,  0.08223945,\n",
      "         0.01360032, -0.00200551],\n",
      "       [-0.06626347,  0.03461789, -0.09381877, ...,  0.08917173,\n",
      "        -0.01112865, -0.05160694]]), 'W_Input': array([[ 0.03324818,  0.01967499,  0.05703908, ...,  0.04314924,\n",
      "        -0.11016764, -0.0436574 ],\n",
      "       [ 0.08157928,  0.05701869, -0.0750379 , ...,  0.02907625,\n",
      "         0.00175879,  0.01633718],\n",
      "       [ 0.04517571,  0.11817931,  0.04185157, ..., -0.00292897,\n",
      "        -0.03184878,  0.01579642],\n",
      "       ...,\n",
      "       [-0.13770719,  0.01294298,  0.01042443, ...,  0.03322916,\n",
      "        -0.04051848, -0.01534496],\n",
      "       [ 0.04452982,  0.02108513,  0.0155317 , ..., -0.00581492,\n",
      "         0.06034651,  0.04854422],\n",
      "       [-0.08804365, -0.01024787, -0.01258122, ..., -0.14809084,\n",
      "        -0.06672318,  0.02944697]]), 'W_Candidate': array([[ 0.10753918, -0.11619441, -0.05413152, ..., -0.0083185 ,\n",
      "        -0.03531611, -0.01089553],\n",
      "       [-0.04819583, -0.08257612, -0.00598925, ...,  0.00631066,\n",
      "        -0.03518126, -0.03223191],\n",
      "       [ 0.01604177, -0.04837716, -0.13449026, ..., -0.00345457,\n",
      "         0.09075587,  0.00179093],\n",
      "       ...,\n",
      "       [ 0.05467543, -0.01440539,  0.05808025, ..., -0.063592  ,\n",
      "        -0.06272254,  0.01610461],\n",
      "       [ 0.02060515, -0.03310312, -0.01020604, ..., -0.05764248,\n",
      "         0.08951308, -0.0225236 ],\n",
      "       [-0.09254257, -0.07048842, -0.0470653 , ..., -0.00401076,\n",
      "        -0.03496412,  0.05540421]]), 'W_Output': array([[-0.0534476 ,  0.02584516,  0.04477084, ..., -0.06684289,\n",
      "         0.03271589, -0.03053377],\n",
      "       [ 0.08288753,  0.00416711,  0.06616922, ...,  0.02825708,\n",
      "         0.04872215,  0.05669956],\n",
      "       [-0.06044451, -0.00136117,  0.10743326, ..., -0.03877087,\n",
      "         0.10458627, -0.12527164],\n",
      "       ...,\n",
      "       [-0.08210475, -0.05871015,  0.02576889, ..., -0.0747129 ,\n",
      "        -0.09460217, -0.04288221],\n",
      "       [ 0.03310667,  0.02051632,  0.04974248, ..., -0.01612011,\n",
      "        -0.06438801, -0.02359327],\n",
      "       [-0.05170707,  0.0384853 ,  0.00513165, ...,  0.09219876,\n",
      "         0.08463142, -0.03252717]]), 'W_stm': array([[-0.05990745, -0.02677495, -0.03888617,  0.02680464,  0.01967541,\n",
      "         0.00795696, -0.06436249, -0.0059412 ,  0.05685982, -0.02983401,\n",
      "         0.03097724, -0.08990412,  0.0653411 , -0.0726131 , -0.00794876,\n",
      "         0.02091808,  0.05549073, -0.06576985, -0.01662861, -0.08014616,\n",
      "        -0.0317984 , -0.03458073, -0.00025429, -0.05260373, -0.01602516,\n",
      "        -0.08032869,  0.07909631, -0.02990306, -0.03370152,  0.00447711,\n",
      "        -0.0255802 ,  0.02097491,  0.07608755,  0.04210295, -0.042803  ,\n",
      "         0.05636518, -0.01216722, -0.04434954, -0.04433042,  0.01364664,\n",
      "         0.12074494,  0.03133969,  0.0312735 , -0.00966447,  0.10508981,\n",
      "        -0.06812216,  0.05216698, -0.04449858,  0.0878526 , -0.12019202,\n",
      "        -0.03657367,  0.06004754, -0.0491265 ,  0.04308652, -0.00758937,\n",
      "         0.02249967, -0.07278949,  0.05278438,  0.02271247, -0.04044216,\n",
      "         0.00301533,  0.09191526,  0.01728933, -0.03588506, -0.02446182,\n",
      "        -0.11777955, -0.05031529,  0.02706573,  0.0348084 , -0.0583357 ,\n",
      "         0.07410096,  0.03651791,  0.07455927, -0.05514503,  0.08703449,\n",
      "         0.10983539, -0.02900169, -0.02553099, -0.01183128, -0.00315487,\n",
      "         0.00641188, -0.00091578, -0.06106659, -0.00359342,  0.05382582,\n",
      "        -0.04565454, -0.02754954, -0.07601953,  0.06897345, -0.05806215,\n",
      "         0.03129982,  0.02330533, -0.11959035,  0.05694185, -0.0086394 ,\n",
      "        -0.09225616,  0.01316069, -0.14235317,  0.16871813,  0.00488738,\n",
      "        -0.03741537, -0.02304936,  0.0138928 , -0.13277982, -0.07669175,\n",
      "         0.0437229 ,  0.01185163,  0.07879752, -0.01408329, -0.03324146,\n",
      "        -0.00139304,  0.03883916, -0.11198816, -0.10681426,  0.01118747,\n",
      "         0.02381128,  0.04398013,  0.00687379,  0.0804661 , -0.01921451,\n",
      "         0.01806978, -0.04271174,  0.00145024, -0.06061265,  0.05616301,\n",
      "        -0.02642929, -0.01868685, -0.00178064, -0.04642388,  0.07957506,\n",
      "         0.02531543, -0.05209228, -0.00923356, -0.0341115 ,  0.08288785,\n",
      "         0.06590728,  0.04184919,  0.06397895,  0.05735929, -0.09884906,\n",
      "        -0.11031813,  0.09954033, -0.03666462, -0.07873563,  0.04258755,\n",
      "         0.0033416 , -0.05996169, -0.04143379,  0.05530724,  0.04016188,\n",
      "        -0.05528399,  0.08934851, -0.06551541,  0.00131617, -0.07549129,\n",
      "        -0.20199757,  0.08027204, -0.00765494,  0.00644667, -0.04436942,\n",
      "         0.08770444,  0.10673543, -0.0004178 , -0.00145108,  0.04302252,\n",
      "        -0.10615149,  0.11011522, -0.00867222,  0.00988766,  0.06503994,\n",
      "         0.05687139,  0.056007  ,  0.0768475 , -0.03068992,  0.05848073,\n",
      "        -0.01234563, -0.02655368,  0.05218306,  0.0018483 ,  0.00555261,\n",
      "         0.00243942, -0.06050369, -0.02733807,  0.06312355,  0.02856075,\n",
      "         0.03623536,  0.03834649,  0.04998214, -0.08092399, -0.02156069,\n",
      "         0.02832663, -0.0325225 ,  0.07905152,  0.0565985 , -0.03260074,\n",
      "        -0.05710191, -0.07426069,  0.05719112,  0.01221575,  0.0390503 ,\n",
      "        -0.08081077,  0.03095916,  0.13196177,  0.0453601 , -0.01153831,\n",
      "         0.04746031, -0.1565076 ,  0.05220558,  0.08268741, -0.03642583,\n",
      "        -0.00070415,  0.06988954, -0.02154502, -0.07826799, -0.02259199,\n",
      "        -0.04690377,  0.03251363,  0.03319228,  0.09051153, -0.02387795,\n",
      "        -0.06007824,  0.04427789, -0.0382789 , -0.07933844,  0.0291879 ,\n",
      "         0.05137395, -0.04274737,  0.00187387, -0.02872909,  0.06420743,\n",
      "        -0.03052842, -0.04810839,  0.04549854, -0.0291256 , -0.1022287 ,\n",
      "         0.04294168, -0.11321264, -0.00252601, -0.02155001,  0.07932999,\n",
      "         0.00604208, -0.06563742, -0.06576561,  0.03750575,  0.02753979,\n",
      "        -0.02359433, -0.04128164, -0.06353809, -0.01335736,  0.00514444,\n",
      "         0.04853338, -0.07931457, -0.04388568, -0.11235908,  0.05133817,\n",
      "        -0.00407736, -0.04276683,  0.09836553,  0.06620027, -0.07069466,\n",
      "        -0.01519401,  0.04220395,  0.00913545,  0.0140408 ,  0.03348812,\n",
      "        -0.01709645, -0.01198513, -0.04078699, -0.00924928, -0.04276685,\n",
      "        -0.02569381, -0.01656496,  0.0362238 ,  0.02048923,  0.05351398,\n",
      "         0.0440069 , -0.0256107 , -0.04828795,  0.03244928,  0.00327269,\n",
      "         0.09370007,  0.07474642,  0.0504131 ,  0.08346979,  0.0921512 ,\n",
      "        -0.07212866,  0.03418685,  0.00721975,  0.01044073,  0.0895525 ,\n",
      "         0.02303357,  0.02561528, -0.08410798, -0.00197948,  0.07180514,\n",
      "        -0.08763128,  0.01508412, -0.0507179 , -0.08017175, -0.04190854]])}, 'bias': {'b_Forget': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'b_Input': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'b_Candidate': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'b_Output': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'b_stm': array([[1.]])}}\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba79ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n",
      " Prediction: 1.0 and truth is True\n",
      " Prediction: 1.0 and truth is False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2500\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, Y_train):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# One-hot encode input and target sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     forward_pass \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforward_pass[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and truth is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[63], line 192\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m forward_pass[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mltm_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ltm_prev\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Calculate output gate\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m o \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW_Output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_Output\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    193\u001b[0m forward_pass[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(o)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Calculate hidden state\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_split(X = X, Y = Y, test_size = 0.20)\n",
    "embed_dim = 12\n",
    "hidden_size = 300\n",
    "batch_size = 32\n",
    "input_size = 2500\n",
    "for inputs, targets in zip(X_train, Y_train):\n",
    "\n",
    "    # One-hot encode input and target sequence\n",
    "    #  inputs_one_hot = dataset.one_hot_encode_sequence(inputs)\n",
    "    #  targets_one_hot = dataset.one_hot_encode_sequence(targets)\n",
    "\n",
    "    # Forward pass\n",
    "    forward_pass = model.forward(inputs)\n",
    "    \n",
    "    \n",
    "    print(f\" Prediction: {forward_pass['result'][-1].squeeze()} and truth is {targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa0ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
